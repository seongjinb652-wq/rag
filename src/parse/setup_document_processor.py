#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Day 2: ë¬¸ì„œ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
ëª©í‘œ: PDF, PPT, Word íŒŒì¼ ìë™ íŒŒì‹± ë° í…ìŠ¤íŠ¸ ì¶”ì¶œ

ê¸°ëŠ¥:
- PDF: í˜ì´ì§€ë³„ ì¶”ì¶œ + OCR (ì´ë¯¸ì§€ í¬í•¨)
- PPT: ìŠ¬ë¼ì´ë“œë³„ í…ìŠ¤íŠ¸ ì¶”ì¶œ
- Word: ë‹¨ë½ë³„ ì¶”ì¶œ
- ì´ë¯¸ì§€: Tesseract OCR

ì‹¤í–‰: python setup_document_processor.py
"""

import os
import sys
from pathlib import Path
from typing import List, Dict, Tuple
import logging
from datetime import datetime

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ
PROJECT_ROOT = Path(__file__).parent.parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

from config import Settings


class DocumentProcessor:
    """ë¬¸ì„œ ì²˜ë¦¬ ì—”ì§„"""
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        self.supported_formats = Settings.SUPPORTED_FORMATS
        self.downloads_dir = Settings.DOWNLOADS_DIR
        self.chunk_size = Settings.CHUNK_SIZE
        self.chunk_overlap = Settings.CHUNK_OVERLAP
        
        logger.info("ğŸ“„ ë¬¸ì„œ ì²˜ë¦¬ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
    
    def process_pdf(self, file_path: Path) -> List[str]:
        """PDF íŒŒì¼ ì²˜ë¦¬"""
        try:
            from PyPDF2 import PdfReader
            
            logger.info(f"ğŸ“• PDF ì²˜ë¦¬: {file_path.name}")
            
            pages = []
            with open(file_path, 'rb') as f:
                reader = PdfReader(f)
                total_pages = len(reader.pages)
                
                for i, page in enumerate(reader.pages):
                    text = page.extract_text()
                    if text.strip():
                        pages.append(f"[Page {i+1}/{total_pages}]\n{text}")
                    
                    if (i + 1) % 10 == 0:
                        logger.info(f"   ì§„í–‰: {i+1}/{total_pages} í˜ì´ì§€")
            
            logger.info(f"âœ… PDF ì™„ë£Œ: {len(pages)}í˜ì´ì§€ ì¶”ì¶œ")
            return pages
        
        except Exception as e:
            logger.error(f"âŒ PDF ì²˜ë¦¬ ì‹¤íŒ¨ ({file_path.name}): {e}")
            return []
    
    def process_pptx(self, file_path: Path) -> List[str]:
        """PowerPoint íŒŒì¼ ì²˜ë¦¬"""
        try:
            from pptx import Presentation
            
            logger.info(f"ğŸ“Š PPT ì²˜ë¦¬: {file_path.name}")
            
            slides = []
            prs = Presentation(file_path)
            total_slides = len(prs.slides)
            
            for i, slide in enumerate(prs.slides):
                text = []
                
                # í…ìŠ¤íŠ¸ ìƒìì—ì„œ ì¶”ì¶œ
                for shape in slide.shapes:
                    if hasattr(shape, "text") and shape.text.strip():
                        text.append(shape.text)
                
                if text:
                    slide_text = f"[Slide {i+1}/{total_slides}]\n" + "\n".join(text)
                    slides.append(slide_text)
                
                if (i + 1) % 10 == 0:
                    logger.info(f"   ì§„í–‰: {i+1}/{total_slides} ìŠ¬ë¼ì´ë“œ")
            
            logger.info(f"âœ… PPT ì™„ë£Œ: {len(slides)}ìŠ¬ë¼ì´ë“œ ì¶”ì¶œ")
            return slides
        
        except Exception as e:
            logger.error(f"âŒ PPT ì²˜ë¦¬ ì‹¤íŒ¨ ({file_path.name}): {e}")
            return []
    
    def process_docx(self, file_path: Path) -> List[str]:
        """Word íŒŒì¼ ì²˜ë¦¬"""
        try:
            from docx import Document
            
            logger.info(f"ğŸ“„ Word ì²˜ë¦¬: {file_path.name}")
            
            paragraphs = []
            doc = Document(file_path)
            total_paras = len(doc.paragraphs)
            
            for i, para in enumerate(doc.paragraphs):
                if para.text.strip():
                    paragraphs.append(para.text)
                
                if (i + 1) % 100 == 0:
                    logger.info(f"   ì§„í–‰: {i+1}/{total_paras} ë‹¨ë½")
            
            logger.info(f"âœ… Word ì™„ë£Œ: {len(paragraphs)}ë‹¨ë½ ì¶”ì¶œ")
            return paragraphs
        
        except Exception as e:
            logger.error(f"âŒ Word ì²˜ë¦¬ ì‹¤íŒ¨ ({file_path.name}): {e}")
            return []
    
    def process_image(self, file_path: Path) -> str:
        """ì´ë¯¸ì§€ íŒŒì¼ ì²˜ë¦¬ (OCR)"""
        try:
            import pytesseract
            from PIL import Image
            
            logger.info(f"ğŸ–¼ï¸ ì´ë¯¸ì§€ OCR: {file_path.name}")
            
            # Tesseract ê²½ë¡œ ì„¤ì • (Windows)
            if sys.platform == 'win32':
                pytesseract.pytesseract_cmd = r'C:\Program Files\Tesseract-OCR\tesseract.exe'
            
            img = Image.open(file_path)
            text = pytesseract.image_to_string(img, lang='kor+eng')
            
            logger.info(f"âœ… OCR ì™„ë£Œ: {len(text)} ê¸€ì")
            return text
        
        except Exception as e:
            logger.error(f"âŒ OCR ì‹¤íŒ¨ ({file_path.name}): {e}")
            return ""
    
    def chunk_text(self, text: str, source: str) -> List[Dict]:
        """í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• """
        from transformers import AutoTokenizer
        
        # í† í¬ë‚˜ì´ì €ë¡œ ì •í™•í•œ í† í° ìˆ˜ ê³„ì‚°
        try:
            tokenizer = AutoTokenizer.from_pretrained(
                'sentence-transformers/xlm-r-base-multilingual-nli-stsb'
            )
        except:
            # í´ë°±: ê°„ë‹¨í•œ ë¶„í• 
            words = text.split()
            chunk_size_words = self.chunk_size // 2
            chunks = []
            
            for i in range(0, len(words), chunk_size_words):
                chunk_words = words[i:i + chunk_size_words]
                chunk_text = ' '.join(chunk_words)
                chunks.append({
                    'text': chunk_text,
                    'source': source,
                    'size': len(chunk_text)
                })
            
            return chunks
        
        # í† í¬ë‚˜ì´ì € ì‚¬ìš©
        tokens = tokenizer.encode(text)
        chunks = []
        
        for i in range(0, len(tokens), self.chunk_size - self.chunk_overlap):
            chunk_tokens = tokens[i:i + self.chunk_size]
            chunk_text = tokenizer.decode(chunk_tokens)
            
            if chunk_text.strip():
                chunks.append({
                    'text': chunk_text,
                    'source': source,
                    'tokens': len(chunk_tokens)
                })
        
        return chunks
    
    def process_directory(self, directory: Path = None) -> Tuple[List[Dict], Dict]:
        """ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  ë¬¸ì„œ ì²˜ë¦¬"""
        if directory is None:
            directory = self.downloads_dir
        
        print("\n" + "="*80)
        print("ğŸ“š ë¬¸ì„œ ì²˜ë¦¬ ì‹œì‘")
        print("="*80)
        
        all_documents = []
        stats = {
            'total_files': 0,
            'processed_files': 0,
            'failed_files': 0,
            'total_chunks': 0,
            'formats': {},
            'start_time': datetime.now()
        }
        
        # ì§€ì›ë˜ëŠ” í˜•ì‹ ì°¾ê¸°
        files = []
        for ext in self.supported_formats:
            files.extend(directory.glob(f'*{ext}'))
        
        stats['total_files'] = len(files)
        logger.info(f"ë°œê²¬ëœ íŒŒì¼: {len(files)}ê°œ")
        
        for file_path in files:
            ext = file_path.suffix.lower()
            
            try:
                contents = []
                
                # í˜•ì‹ë³„ ì²˜ë¦¬
                if ext == '.pdf':
                    contents = self.process_pdf(file_path)
                elif ext == '.pptx':
                    contents = self.process_pptx(file_path)
                elif ext == '.docx':
                    contents = self.process_docx(file_path)
                elif ext in {'.png', '.jpg', '.jpeg'}:
                    text = self.process_image(file_path)
                    contents = [text] if text else []
                elif ext == '.txt':
                    with open(file_path, 'r', encoding='utf-8') as f:
                        contents = [f.read()]
                
                # ì²­í¬ ë¶„í• 
                if contents:
                    for content in contents:
                        chunks = self.chunk_text(content, str(file_path))
                        all_documents.extend(chunks)
                    
                    stats['processed_files'] += 1
                    stats['formats'][ext] = stats['formats'].get(ext, 0) + 1
                    stats['total_chunks'] += len([c for c in all_documents 
                                                  if str(c['source']) == str(file_path)])
                else:
                    stats['failed_files'] += 1
            
            except Exception as e:
                logger.error(f"íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨ ({file_path.name}): {e}")
                stats['failed_files'] += 1
        
        # ê²°ê³¼ ì¶œë ¥
        stats['end_time'] = datetime.now()
        stats['duration'] = (stats['end_time'] - stats['start_time']).total_seconds()
        
        print("\n" + "="*80)
        print("ğŸ“Š ì²˜ë¦¬ ê²°ê³¼")
        print("="*80)
        print(f"ì´ íŒŒì¼: {stats['total_files']}")
        print(f"ì„±ê³µ: {stats['processed_files']}")
        print(f"ì‹¤íŒ¨: {stats['failed_files']}")
        print(f"ìƒì„±ëœ ì²­í¬: {stats['total_chunks']}")
        print(f"í˜•ì‹ë³„: {stats['formats']}")
        print(f"ì†Œìš” ì‹œê°„: {stats['duration']:.2f}ì´ˆ")
        print("="*80 + "\n")
        
        return all_documents, stats


def main():
    """í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    processor = DocumentProcessor()
    
    # ìƒ˜í”Œ íŒŒì¼ ìƒì„±
    test_file = processor.downloads_dir / "test.txt"
    test_file.parent.mkdir(parents=True, exist_ok=True)
    
    with open(test_file, 'w', encoding='utf-8') as f:
        f.write("Day 2 í…ŒìŠ¤íŠ¸\n")
        f.write("ì´ê²ƒì€ ë¬¸ì„œ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ì…ë‹ˆë‹¤.\n")
        f.write("PDF, PPT, Word, ì´ë¯¸ì§€ ë“±ì„ ìë™ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.\n")
    
    # ì²˜ë¦¬ ì‹¤í–‰
    documents, stats = processor.process_directory()
    
    print(f"âœ… Day 2 ë¬¸ì„œ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print(f"   ì²˜ë¦¬ëœ ì²­í¬: {len(documents)}")
    
    # ì •ë¦¬
    test_file.unlink()


if __name__ == "__main__":
    main()
