#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ë¡œì»¬ ë°°ì¹˜ ì²˜ë¦¬ê¸° - ë¡œì»¬ ë””ë ‰í† ë¦¬ íŒŒì¼ ìŠ¤ìº” ë° ì²˜ë¦¬
ëª©í‘œ: ë¡œì»¬ ë””ë ‰í† ë¦¬ íŒŒì¼ â†’ ì²˜ë¦¬ â†’ ë²¡í„° DB ì—…ë°ì´íŠ¸
ë‚ ì§œ: 2026-01-26

ì‹¤í–‰: python local_batch_scheduler.py
"""

import os
import sys
from pathlib import Path
from typing import List, Dict, Tuple
import logging
import json
from datetime import datetime
import hashlib

# ì„¤ì •
class Settings:
    TARGET_DIR = Path(r"C:/Users/USER/Downloads/@@@ì¸ë„ë„¤ì‹œì•„PDTì•”ì„¼í„°FS")
    STATE_FILE = Path("batch_state_local.json")
    LOG_DIR = Path("logs")
    SUPPORTED_FORMATS = {'.pdf', '.docx', '.doc', '.pptx', '.txt'}

# ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
Settings.LOG_DIR.mkdir(parents=True, exist_ok=True)

# ë¡œê·¸ íŒŒì¼ ê²½ë¡œ
log_file = Settings.LOG_DIR / 'batch.log'

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(str(log_file), encoding='utf-8', mode='a'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)
logger.info(f"ğŸ“ ë¡œê·¸ íŒŒì¼: {log_file}")


class LocalBatchProcessor:
    """ë¡œì»¬ ë°°ì¹˜ ì²˜ë¦¬ ì—”ì§„"""
    
    BATCH_SIZE = 50
    SAMPLE_SIZE = 5
    
    def __init__(self):
        logger.info("ğŸ”§ ë¡œì»¬ ë°°ì¹˜ ì²˜ë¦¬ê¸° ì´ˆê¸°í™”...")
        
        self.target_dir = Settings.TARGET_DIR
        self.state_file = Settings.STATE_FILE
        self.state = self._load_state()
        
        self.stats = {
            'start_time': None,
            'end_time': None,
            'scanned_files': 0,
            'new_files': 0,
            'modified_files': 0,
            'deleted_files': 0,
            'processed_files': 0,
            'failed_files': 0,
            'total_chunks': 0,
            'errors': []
        }
    
    def _load_state(self) -> Dict:
        """ìƒíƒœ íŒŒì¼ ë¡œë“œ"""
        if self.state_file.exists():
            with open(self.state_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        
        return {
            'processed_files': {},
            'last_run': None,
            'total_chunks': 0
        }
    
    def _save_state(self):
        """ìƒíƒœ íŒŒì¼ ì €ì¥"""
        with open(self.state_file, 'w', encoding='utf-8') as f:
            json.dump(self.state, f, ensure_ascii=False, indent=2)
        logger.info(f"âœ… ìƒíƒœ ì €ì¥: {self.state_file}")
    
    def _calculate_file_hash(self, file_path: Path) -> str:
        """íŒŒì¼ í•´ì‹œ ê³„ì‚°"""
        sha256_hash = hashlib.sha256()
        with open(file_path, "rb") as f:
            for byte_block in iter(lambda: f.read(4096), b""):
                sha256_hash.update(byte_block)
        return sha256_hash.hexdigest()
    
    def scan_files(self) -> List[Dict]:
        """ë¡œì»¬ ë””ë ‰í† ë¦¬ ì¬ê·€ ìŠ¤ìº”"""
        
        logger.info("ğŸ” ë¡œì»¬ íŒŒì¼ ìŠ¤ìº” ì‹œì‘ (í•˜ìœ„ ë””ë ‰í† ë¦¬ í¬í•¨)...")
        
        if not self.target_dir.exists():
            logger.error(f"âŒ ë””ë ‰í† ë¦¬ ì—†ìŒ: {self.target_dir}")
            return []
        
        files = []
        
        for root, dirs, filenames in os.walk(self.target_dir):
            for fname in filenames:
                file_path = Path(root) / fname
                
                # ì§€ì› í˜•ì‹ë§Œ
                if file_path.suffix.lower() in Settings.SUPPORTED_FORMATS:
                    try:
                        stat = file_path.stat()
                        files.append({
                            'name': fname,
                            'path': str(file_path),
                            'size': stat.st_size,
                            'modified': datetime.fromtimestamp(stat.st_mtime).isoformat()
                        })
                        
                        if len(files) % 100 == 0:
                            logger.info(f"   ìŠ¤ìº” ì¤‘... {len(files)}ê°œ íŒŒì¼ ë°œê²¬")
                    
                    except Exception as e:
                        logger.warning(f"âš ï¸ íŒŒì¼ ì •ë³´ ì½ê¸° ì‹¤íŒ¨: {fname} - {e}")
        
        self.stats['scanned_files'] = len(files)
        logger.info(f"âœ… ìŠ¤ìº” ì™„ë£Œ: {len(files)}ê°œ íŒŒì¼ ë°œê²¬")
        
        # ë””ë ‰í† ë¦¬ë³„ í†µê³„
        dir_counts = {}
        for file_info in files:
            dir_path = str(Path(file_info['path']).parent)
            dir_counts[dir_path] = dir_counts.get(dir_path, 0) + 1
        
        logger.info(f"ğŸ“ ë””ë ‰í† ë¦¬ë³„ íŒŒì¼ ë¶„í¬:")
        for dir_path, count in sorted(dir_counts.items()):
            logger.info(f"   {dir_path}: {count}ê°œ")
        
        return files
    
    def classify_files(self, all_files: List[Dict]) -> Tuple[List[Dict], List[Dict], List[Dict]]:
        """íŒŒì¼ ë¶„ë¥˜"""
        
        logger.info("ğŸ“Š íŒŒì¼ ìƒíƒœ ë¶„ë¥˜...")
        
        cloud_files = {f['path'] for f in all_files}
        stored_files = set(self.state['processed_files'].keys())
        
        new_files = []
        modified_files = []
        deleted_files = list(stored_files - cloud_files)
        
        # ì‹ ê·œ
        for file_info in all_files:
            if file_info['path'] not in self.state['processed_files']:
                new_files.append(file_info)
                logger.info(f"   âœ¨ ì‹ ê·œ: {file_info['path']}")
        
        # ì‚­ì œ
        if deleted_files:
            logger.warning(f"âš ï¸ ì‚­ì œëœ íŒŒì¼ ê°ì§€: {len(deleted_files)}ê°œ")
            for deleted in deleted_files:
                logger.warning(f"   âš ï¸ ì‚­ì œ: {deleted}")
            self.stats['deleted_files'] = len(deleted_files)
        
        # ìˆ˜ì •
        for file_info in all_files:
            if file_info['path'] in self.state['processed_files']:
                stored_size = self.state['processed_files'][file_info['path']].get('file_size', 0)
                if file_info['size'] != stored_size:
                    modified_files.append(file_info)
                    logger.info(f"   ğŸ“ ìˆ˜ì •: {file_info['path']} (í¬ê¸° ë³€ê²½: {stored_size} â†’ {file_info['size']})")
        
        self.stats['new_files'] = len(new_files)
        self.stats['modified_files'] = len(modified_files)
        
        logger.info(
            f"ğŸ“Š ë¶„ë¥˜ ì™„ë£Œ: "
            f"ì‹ ê·œ {len(new_files)}ê°œ, "
            f"ìˆ˜ì • {len(modified_files)}ê°œ, "
            f"ì‚­ì œ {len(deleted_files)}ê°œ"
        )
        
        return new_files, modified_files, deleted_files
    
    def process_files(self, files: List[Dict]) -> int:
        """íŒŒì¼ ì²˜ë¦¬ - DocumentProcessor + VectorStore ì—°ë™"""
        
        logger.info(f"ğŸ“¦ íŒŒì¼ ì²˜ë¦¬ ì‹œì‘: {len(files)}ê°œ")
        
        # DocumentProcessor ì„í¬íŠ¸
        sys.path.insert(0, str(Path(__file__).parent.parent / 'parse'))
        from setup_document_processor import DocumentProcessor
        
        # VectorStore ì„í¬íŠ¸
        sys.path.insert(0, str(Path(__file__).parent.parent / 'embed'))
        from setup_vector_store import VectorStore
        
        processor = DocumentProcessor()
        vector_store = VectorStore()
        
        processed = 0
        total_chunks = 0
        
        for i, file_info in enumerate(files, 1):
            file_path = Path(file_info['path'])
            
            if not file_path.exists():
                logger.warning(f"âš ï¸ íŒŒì¼ ì—†ìŒ: {file_path}")
                continue
            
            try:
                ext = file_path.suffix.lower()
                contents = []
                
                # í˜•ì‹ë³„ ì²˜ë¦¬
                if ext == '.pdf':
                    contents = processor.process_pdf(file_path)
                elif ext == '.pptx':
                    contents = processor.process_pptx(file_path)
                elif ext in {'.docx', '.doc'}:
                    contents = processor.process_docx(file_path)
                elif ext in {'.png', '.jpg', '.jpeg'}:
                    text = processor.process_image(file_path)
                    contents = [text] if text else []
                elif ext == '.txt':
                    with open(file_path, 'r', encoding='utf-8') as f:
                        contents = [f.read()]
                
                # ì²­í¬ ìƒì„±
                chunks = []
                if contents:
                    for content in contents:
                        chunks.extend(processor.chunk_text(content, str(file_path)))
                
                # ë²¡í„° DBì— ì €ì¥
                if chunks:
                    result = vector_store.add_documents(chunks)
                    logger.info(f"   ğŸ’¾ ë²¡í„° ì €ì¥: {result['added']}ê°œ ì²­í¬")
                
                # íŒŒì¼ í•´ì‹œ ê³„ì‚°
                file_hash = self._calculate_file_hash(file_path)
                
                # ìƒíƒœ ì—…ë°ì´íŠ¸
                self.state['processed_files'][file_info['path']] = {
                    'modified_time': file_info['modified'],
                    'file_hash': file_hash,
                    'file_size': file_info['size'],
                    'chunks': len(chunks),
                    'status': 'processed'
                }
                
                processed += 1
                total_chunks += len(chunks)
                self.stats['processed_files'] += 1
                self.stats['total_chunks'] += len(chunks)
                
                if i % 5 == 0:
                    logger.info(f"   ì§„í–‰: {i}/{len(files)} ì²˜ë¦¬, {total_chunks}ê°œ ì²­í¬")
            
            except Exception as e:
                logger.error(f"âŒ ì²˜ë¦¬ ì‹¤íŒ¨ ({file_info['name']}): {e}")
                self.stats['failed_files'] += 1
                self.stats['errors'].append(f"ì²˜ë¦¬ ì‹¤íŒ¨: {file_info['name']}")
        
        logger.info(f"âœ… ì²˜ë¦¬ ì™„ë£Œ: {processed}ê°œ íŒŒì¼, {total_chunks}ê°œ ì²­í¬")
        
        # ë²¡í„° DB í†µê³„ ì €ì¥
        vector_store.save_stats()
        
        return processed
    
    def generate_report(self):
        """ë¦¬í¬íŠ¸ ìƒì„±"""
        
        self.stats['end_time'] = datetime.now().isoformat()
        
        elapsed = (datetime.fromisoformat(self.stats['end_time']) - 
                  datetime.fromisoformat(self.stats['start_time'])).total_seconds()
        
        report = f"""
{'='*80}
ğŸ“Š ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ ë¦¬í¬íŠ¸
{'='*80}

â±ï¸ ì†Œìš” ì‹œê°„: {elapsed:.2f}ì´ˆ

ğŸ“ˆ ì²˜ë¦¬ í†µê³„:
   âœ“ ìŠ¤ìº” íŒŒì¼: {self.stats['scanned_files']}ê°œ
   âœ¨ ì‹ ê·œ íŒŒì¼: {self.stats['new_files']}ê°œ
   ğŸ“ ìˆ˜ì • íŒŒì¼: {self.stats['modified_files']}ê°œ
   âš ï¸ ì‚­ì œ íŒŒì¼: {self.stats['deleted_files']}ê°œ (ê°ì§€ë§Œ)
   âœ“ ì²˜ë¦¬ ì„±ê³µ: {self.stats['processed_files']}ê°œ
   âœ— ì²˜ë¦¬ ì‹¤íŒ¨: {self.stats['failed_files']}ê°œ

ğŸ—“ï¸ ì‹¤í–‰ ì‹œê°„:
   ì‹œì‘: {self.stats['start_time']}
   ì¢…ë£Œ: {self.stats['end_time']}

{'='*80}
"""
        
        if self.stats['errors']:
            report += f"\nâš ï¸ ì˜¤ë¥˜ ëª©ë¡:\n"
            for error in self.stats['errors']:
                report += f"   - {error}\n"
        
        report += f"\n{'='*80}\n"
        
        logger.info(report)
        
        # ë¦¬í¬íŠ¸ ì €ì¥
        report_file = Settings.LOG_DIR / f"batch_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
        
        logger.info(f"âœ… ë¦¬í¬íŠ¸ ì €ì¥: {report_file}")
    
    def run(self):
        """ë°°ì¹˜ ì‹¤í–‰"""
        
        self.stats['start_time'] = datetime.now().isoformat()
        
        logger.info("\n" + "="*80)
        logger.info("ğŸš€ ë¡œì»¬ ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘")
        logger.info("="*80)
        
        try:
            # 1. ìŠ¤ìº”
            all_files = self.scan_files()
            if not all_files:
                logger.warning("ì²˜ë¦¬í•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")
                return
            
            # 2. ë¶„ë¥˜
            new_files, modified_files, deleted_files = self.classify_files(all_files)
            
            files_to_process = new_files + modified_files
            
            if not files_to_process:
                logger.info("ì²˜ë¦¬í•  ìƒˆ íŒŒì¼ì´ë‚˜ ìˆ˜ì • íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")
                if deleted_files:
                    logger.warning(f"âš ï¸ ì‚­ì œëœ íŒŒì¼ {len(deleted_files)}ê°œëŠ” ë¯¸ì²˜ë¦¬")
                return
            
            # 3. ì²˜ë¦¬
            processed = self.process_files(files_to_process)
            
            # 4. ìƒíƒœ ì €ì¥
            self.state['last_run'] = datetime.now().isoformat()
            self._save_state()
            
            # 5. ë¦¬í¬íŠ¸
            self.generate_report()
        
        except Exception as e:
            logger.error(f"âŒ ë°°ì¹˜ ì‹¤íŒ¨: {e}", exc_info=True)
            self.stats['errors'].append(f"ë°°ì¹˜ ì‹¤íŒ¨: {e}")


def main():
    batch = LocalBatchProcessor()
    batch.run()


if __name__ == "__main__":
    main()
