#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ë°°ì¹˜ ìë™í™” - ì›”ê°„ ë¬¸ì„œ ì²˜ë¦¬ (ì‹ ê·œ/ìˆ˜ì •/ì‚­ì œ ê°ì§€)
ëª©í‘œ: ë„¤ì´ë²„ í´ë¼ìš°ë“œì—ì„œ íŒŒì¼ ë‹¤ìš´ë¡œë“œ â†’ ì²˜ë¦¬ â†’ ë²¡í„° DB ì—…ë°ì´íŠ¸
ì†Œìœ ì : ì„±ì§„
ë‚ ì   : 2026-01-26

ë¡œì§:
1ï¸âƒ£ íŒŒì¼ ìŠ¤ìº” (ê°œìˆ˜ íŒŒì•…) - í•˜ìœ„ ë””ë ‰í† ë¦¬ ì¬ê·€ íƒìƒ‰ ì¶”ê°€ âœ…
2ï¸âƒ£ íŒŒì¼ ìƒíƒœ ë¶„ë¥˜
   - ì‹ ê·œ: ì²˜ë¦¬ í•„ìš”
   - ìˆ˜ì •: ì¬ì²˜ë¦¬ í•„ìš” âœ… êµ¬í˜„ë¨
   - ì‚­ì œ: ì•ˆì „ ì°¨ì› ì œì™¸ (ë°ì´í„° ë¬´ê²°ì„±) âš ï¸ ì°¨ë‹¨ë¨
3ï¸âƒ£ ìƒ˜í”Œ í…ŒìŠ¤íŠ¸ (2~5ê°œ)
4ï¸âƒ£ ë°°ì¹˜ ì²˜ë¦¬ (50ê°œ ë‹¨ìœ„)
5ï¸âƒ£ ìµœì¢… ë¦¬í¬íŠ¸

ì‹¤í–‰: python setup_batch_scheduler.py
ìŠ¤ì¼€ì¤„: ë§¤ì›” 1ì¼ ì˜¤ì „ 2ì‹œ (APScheduler)
"""

import os
import sys
from pathlib import Path
from typing import List, Dict, Tuple
import logging
import json
from datetime import datetime
import boto3
import hashlib

# config.py ë¡œë“œ
PROJECT_ROOT = Path(__file__).parent.parent
config_file = PROJECT_ROOT / 'config.py'

import importlib.util
spec = importlib.util.spec_from_file_location("config", config_file)
config_module = importlib.util.module_from_spec(spec)
spec.loader.exec_module(config_module)

Settings = config_module.Settings

# ë¡œê¹…
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(Settings.LOGS_DIR / 'batch.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class BatchProcessor:
    """ë°°ì¹˜ ì²˜ë¦¬ ì—”ì§„ - ì‹ ê·œ, ìˆ˜ì •, ì‚­ì œ ê°ì§€ í¬í•¨"""
    
    BATCH_SIZE = 50  # í•œ ë²ˆì— ì²˜ë¦¬í•  íŒŒì¼ ê°œìˆ˜
    SAMPLE_SIZE = 5  # í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ ê°œìˆ˜
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        logger.info("ğŸ”§ ë°°ì¹˜ ì²˜ë¦¬ê¸° ì´ˆê¸°í™”...")
        
        # Naver Cloud ì—°ê²°
        self.s3_client = boto3.client(
            's3',
            endpoint_url=Settings.NAVER_ENDPOINT,
            aws_access_key_id=Settings.NAVER_ACCESS_KEY,
            aws_secret_access_key=Settings.NAVER_SECRET_KEY,
            region_name=Settings.NAVER_REGION
        )
        
        logger.info("âœ… ë„¤ì´ë²„ í´ë¼ìš°ë“œ ì—°ê²° ì™„ë£Œ")
        
        # ìƒíƒœ íŒŒì¼
        self.state_file = Settings.BATCH_STATE_FILE
        self.state = self._load_state()
        
        # ì²˜ë¦¬ í†µê³„
        self.stats = {
            'start_time': None,
            'end_time': None,
            'scanned_files': 0,
            'new_files': 0,
            'modified_files': 0,
            'deleted_files': 0,  # âš ï¸ ê°ì§€ë§Œ í•˜ê³  ì²˜ë¦¬ ì•ˆ í•¨
            'downloaded_files': 0,
            'processed_files': 0,
            'failed_files': 0,
            'total_chunks': 0,
            'errors': []
        }
    
    def _load_state(self) -> Dict:
        """ìƒíƒœ íŒŒì¼ ë¡œë“œ
        
        ìƒíƒœ íŒŒì¼ êµ¬ì¡°:
        {
            "processed_files": {
                "dir1/dir2/file1.pdf": {
                    "modified_time": "2025-01-26T14:30:00",
                    "file_hash": "abc123def456",
                    "file_size": 5120,
                    "chunks": 250,
                    "status": "processed"
                }
            },
            "last_run": "2025-01-26T14:30:00",
            "total_chunks": 1000
        }
        """
        if self.state_file.exists():
            with open(self.state_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        
        # ì´ˆê¸° ìƒíƒœ êµ¬ì¡°
        return {
            'processed_files': {},
            'last_run': None,
            'total_chunks': 0
        }
    
    def _save_state(self):
        """ìƒíƒœ íŒŒì¼ ì €ì¥"""
        self.state_file.parent.mkdir(parents=True, exist_ok=True)
        with open(self.state_file, 'w', encoding='utf-8') as f:
            json.dump(self.state, f, ensure_ascii=False, indent=2)
        logger.info(f"âœ… ìƒíƒœ ì €ì¥: {self.state_file}")
    
    def _calculate_file_hash(self, file_path: Path) -> str:
        """íŒŒì¼ í•´ì‹œ ê³„ì‚° (íŒŒì¼ ìˆ˜ì • ê°ì§€ìš©)"""
        sha256_hash = hashlib.sha256()
        with open(file_path, "rb") as f:
            for byte_block in iter(lambda: f.read(4096), b""):
                sha256_hash.update(byte_block)
        return sha256_hash.hexdigest()
    
    def scan_files(self) -> List[Dict]:
        """Naver Cloudì—ì„œ íŒŒì¼ ëª©ë¡ ìŠ¤ìº” + ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘ (ì¬ê·€ì )
        
        ê°œì„ ì‚¬í•­:
        - í•˜ìœ„ ë””ë ‰í† ë¦¬ ì¬ê·€ì  íƒìƒ‰
        - í˜ì´ì§€ë„¤ì´ì…˜ ì§€ì› (1000ê°œ ì´ìƒ íŒŒì¼)
        - ë””ë ‰í† ë¦¬ë³„ íŒŒì¼ ë¶„í¬ í†µê³„
        
        ë°˜í™˜: [
            {
                'name': 'file1.pdf',
                'path': 'dir1/dir2/file1.pdf',
                'size': 5120,
                'modified': '2025-01-26T14:30:00'
            }
        ]
        """
        
        logger.info("ğŸ” íŒŒì¼ ìŠ¤ìº” ì‹œì‘ (í•˜ìœ„ ë””ë ‰í† ë¦¬ í¬í•¨)...")
        
        try:
            files = []
            continuation_token = None
            
            # S3ì˜ ëª¨ë“  ê°ì²´ë¥¼ í˜ì´ì§€ë„¤ì´ì…˜ìœ¼ë¡œ ê°€ì ¸ì˜¤ê¸°
            while True:
                # list_objects_v2 íŒŒë¼ë¯¸í„° ì„¤ì •
                list_params = {
                    'Bucket': Settings.NAVER_BUCKET_NAME,
                    'MaxKeys': 1000  # í•œ ë²ˆì— ìµœëŒ€ 1000ê°œ
                }
                
                if continuation_token:
                    list_params['ContinuationToken'] = continuation_token
                
                response = self.s3_client.list_objects_v2(**list_params)
                
                # íŒŒì¼ ì •ë³´ ìˆ˜ì§‘
                if 'Contents' in response:
                    for obj in response['Contents']:
                        # ë””ë ‰í† ë¦¬ê°€ ì•„ë‹Œ ì‹¤ì œ íŒŒì¼ë§Œ ì¶”ê°€ (ëì´ /ê°€ ì•„ë‹Œ ê²ƒ)
                        if not obj['Key'].endswith('/'):
                            files.append({
                                'name': obj['Key'].split('/')[-1],  # íŒŒì¼ëª…ë§Œ
                                'path': obj['Key'],  # ì „ì²´ ê²½ë¡œ
                                'size': obj['Size'],
                                'modified': obj['LastModified'].isoformat()
                            })
                            
                            # ì§„í–‰ ìƒí™© ë¡œê¹… (100ê°œë§ˆë‹¤)
                            if len(files) % 100 == 0:
                                logger.info(f"   ìŠ¤ìº” ì¤‘... {len(files)}ê°œ íŒŒì¼ ë°œê²¬")
                
                # ë‹¤ìŒ í˜ì´ì§€ê°€ ìˆëŠ”ì§€ í™•ì¸
                if response.get('IsTruncated', False):
                    continuation_token = response.get('NextContinuationToken')
                else:
                    break
            
            self.stats['scanned_files'] = len(files)
            logger.info(f"âœ… ìŠ¤ìº” ì™„ë£Œ: {len(files)}ê°œ íŒŒì¼ ë°œê²¬")
            
            # ë””ë ‰í† ë¦¬ë³„ íŒŒì¼ ê°œìˆ˜ ì¶œë ¥
            dir_counts = {}
            for file_info in files:
                dir_path = '/'.join(file_info['path'].split('/')[:-1])
                if not dir_path:
                    dir_path = '(ë£¨íŠ¸)'
                dir_counts[dir_path] = dir_counts.get(dir_path, 0) + 1
            
            logger.info(f"ğŸ“ ë””ë ‰í† ë¦¬ë³„ íŒŒì¼ ë¶„í¬:")
            for dir_path, count in sorted(dir_counts.items()):
                logger.info(f"   {dir_path}: {count}ê°œ")
            
            return files
        
        except Exception as e:
            logger.error(f"âŒ ìŠ¤ìº” ì‹¤íŒ¨: {e}")
            self.stats['errors'].append(f"ìŠ¤ìº” ì‹¤íŒ¨: {e}")
            return []
    
    def classify_files(self, all_files: List[Dict]) -> Tuple[List[Dict], List[Dict], List[Dict]]:
        """íŒŒì¼ì„ ìƒíƒœë³„ë¡œ ë¶„ë¥˜
        
        ê°œì„ ì‚¬í•­:
        - ì „ì²´ ê²½ë¡œ(path)ë¥¼ í‚¤ë¡œ ì‚¬ìš©í•˜ì—¬ íŒŒì¼ ì¶”ì 
        
        ë¶„ë¥˜:
        - ì‹ ê·œ: ìƒíƒœì— ì—†ëŠ” íŒŒì¼
        - ìˆ˜ì •: í•´ì‹œê°’ì´ ë‹¤ë¥¸ íŒŒì¼
        - ì‚­ì œ: ìƒíƒœì—ëŠ” ìˆì§€ë§Œ í´ë¼ìš°ë“œì— ì—†ëŠ” íŒŒì¼
        
        ë°˜í™˜: (new_files, modified_files, deleted_files)
        """
        
        logger.info("ğŸ“Š íŒŒì¼ ìƒíƒœ ë¶„ë¥˜...")
        
        # í´ë¼ìš°ë“œì˜ íŒŒì¼ ê²½ë¡œ set
        cloud_files = {f['path'] for f in all_files}
        
        # ìƒíƒœì— ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ set
        stored_files = set(self.state['processed_files'].keys())
        
        new_files = []
        modified_files = []
        deleted_files = list(stored_files - cloud_files)
        
        # ì‹ ê·œ íŒŒì¼ ì°¾ê¸°
        for file_info in all_files:
            if file_info['path'] not in self.state['processed_files']:
                new_files.append(file_info)
                logger.info(f"   âœ¨ ì‹ ê·œ: {file_info['path']}")
        
        # âš ï¸ ì‚­ì œ íŒŒì¼ ê°ì§€ (ì²˜ë¦¬í•˜ì§€ ì•ŠìŒ - ë°ì´í„° ë¬´ê²°ì„± ë³´í˜¸)
        if deleted_files:
            logger.warning(f"âš ï¸ ì‚­ì œëœ íŒŒì¼ ê°ì§€: {len(deleted_files)}ê°œ")
            for deleted in deleted_files:
                logger.warning(f"   âš ï¸ ì‚­ì œ: {deleted}")
                # ì£¼ì„: ì‚­ì œ íŒŒì¼ì€ ë²¡í„° DBì—ì„œ ì œê±°í•˜ì§€ ì•ŠìŒ
                # ì´ìœ : ë²¡í„° DBì—ì„œ íŠ¹ì • ì²­í¬ë§Œ ì œê±°í•˜ê¸° ì–´ë ¤ì›€
                # í–¥í›„: ì‚­ì œ íŒŒì¼ ì¶”ì ì„ ìœ„í•´ ìƒíƒœ íŒŒì¼ì—ë§Œ ê¸°ë¡
            self.stats['deleted_files'] = len(deleted_files)
        
        # ìˆ˜ì • íŒŒì¼ ì°¾ê¸° (íŒŒì¼ í¬ê¸° ë¹„êµ)
        for file_info in all_files:
            if file_info['path'] in self.state['processed_files']:
                stored_size = self.state['processed_files'][file_info['path']].get('file_size', 0)
                if file_info['size'] != stored_size:
                    modified_files.append(file_info)
                    logger.info(f"   ğŸ“ ìˆ˜ì •: {file_info['path']} (í¬ê¸° ë³€ê²½: {stored_size} â†’ {file_info['size']})")
        
        self.stats['new_files'] = len(new_files)
        self.stats['modified_files'] = len(modified_files)
        
        logger.info(
            f"ğŸ“Š ë¶„ë¥˜ ì™„ë£Œ: "
            f"ì‹ ê·œ {len(new_files)}ê°œ, "
            f"ìˆ˜ì • {len(modified_files)}ê°œ, "
            f"ì‚­ì œ {len(deleted_files)}ê°œ"
        )
        
        return new_files, modified_files, deleted_files
    
    def download_files(self, files: List[Dict]) -> List[Path]:
        """Naver Cloudì—ì„œ íŒŒì¼ ë‹¤ìš´ë¡œë“œ
        
        ê°œì„ ì‚¬í•­:
        - ì „ì²´ ê²½ë¡œ(path)ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¤ìš´ë¡œë“œ
        - ë””ë ‰í† ë¦¬ êµ¬ì¡° ìœ ì§€
        """
        
        logger.info(f"â¬‡ï¸ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹œì‘: {len(files)}ê°œ")
        
        downloads_dir = Settings.DOWNLOADS_DIR
        downloads_dir.mkdir(parents=True, exist_ok=True)
        
        downloaded = []
        
        for i, file_info in enumerate(files, 1):
            file_key = file_info['path']  # ì „ì²´ ê²½ë¡œ ì‚¬ìš©
            try:
                # ë¡œì»¬ ê²½ë¡œ ì„¤ì • (ë””ë ‰í† ë¦¬ êµ¬ì¡° ìœ ì§€)
                local_path = downloads_dir / file_key.replace('/', '_')
                
                self.s3_client.download_file(
                    Settings.NAVER_BUCKET_NAME,
                    file_key,
                    str(local_path)
                )
                
                downloaded.append(local_path)
                self.stats['downloaded_files'] += 1
                
                if i % 10 == 0:
                    logger.info(f"   ì§„í–‰: {i}/{len(files)} ë‹¤ìš´ë¡œë“œ")
            
            except Exception as e:
                logger.error(f"âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ ({file_key}): {e}")
                self.stats['errors'].append(f"ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {file_key}")
                self.stats['failed_files'] += 1
        
        logger.info(f"âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {len(downloaded)}ê°œ")
        
        return downloaded
    
    def test_sample(self, files: List[Path]) -> bool:
        """ìƒ˜í”Œ íŒŒì¼ë¡œ í…ŒìŠ¤íŠ¸
        
        ëª©ì : ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì •ìƒ ì‘ë™ í™•ì¸
        """
        
        if len(files) < self.SAMPLE_SIZE:
            sample_files = files
        else:
            sample_files = files[:self.SAMPLE_SIZE]
        
        logger.info(f"ğŸ§ª ìƒ˜í”Œ í…ŒìŠ¤íŠ¸: {len(sample_files)}ê°œ íŒŒì¼")
        
        # Document Processor ì„í¬íŠ¸
        # sys.path.insert(0, str(PROJECT_ROOT / 'src' / 'parse'))
        sys.path.insert(0, str(PROJECT_ROOT / 'parse'))
        from setup_document_processor import DocumentProcessor
        
        processor = DocumentProcessor()
        
        try:
            for file_path in sample_files:
                if not file_path.exists():
                    continue
                
                # íŒŒì¼ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
                ext = file_path.suffix.lower()
                
                if ext == '.pdf':
                    processor.process_pdf(file_path)
                elif ext == '.pptx':
                    processor.process_pptx(file_path)
                elif ext == '.docx':
                    processor.process_docx(file_path)
                elif ext in {'.png', '.jpg', '.jpeg'}:
                    processor.process_image(file_path)
            
            logger.info(f"âœ… ìƒ˜í”Œ í…ŒìŠ¤íŠ¸ í†µê³¼")
            
            return True
        
        except Exception as e:
            logger.error(f"âŒ ìƒ˜í”Œ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            self.stats['errors'].append(f"ìƒ˜í”Œ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            
            return False
    
    def process_batch(self, files: List[Path], files_info: List[Dict]) -> Tuple[int, int]:
        """ë°°ì¹˜ ì²˜ë¦¬ (50ê°œì”©)
        
        ì‹ ê·œ + ìˆ˜ì • íŒŒì¼ì„ ì²˜ë¦¬í•˜ê³  ë²¡í„° DBì— ì¶”ê°€
        
        ê°œì„ ì‚¬í•­:
        - ì „ì²´ ê²½ë¡œ(path)ë¥¼ í‚¤ë¡œ ì‚¬ìš©í•˜ì—¬ ìƒíƒœ ì €ì¥
        """
        
        logger.info(f"ğŸ“¦ ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘: {len(files)}ê°œ íŒŒì¼")
        
        # Document Processor ì„í¬íŠ¸
        # sys.path.insert(0, str(PROJECT_ROOT / 'src' / 'parse'))
        sys.path.insert(0, str(PROJECT_ROOT / 'parse'))
        from setup_document_processor import DocumentProcessor
        
        # Vector Store ì„í¬íŠ¸
        # sys.path.insert(0, str(PROJECT_ROOT / 'src' / 'embed'))
        sys.path.insert(0, str(PROJECT_ROOT / 'embed'))
        from setup_vector_store import VectorStore
        
        processor = DocumentProcessor()
        vector_store = VectorStore()
        
        total_chunks = 0
        batch_num = 1
        
        # íŒŒì¼ ì •ë³´ ë§µ (ê²½ë¡œ â†’ ì •ë³´)
        file_info_map = {info['path']: info for info in files_info}
        
        # ë°°ì¹˜ ì²˜ë¦¬
        for i in range(0, len(files), self.BATCH_SIZE):
            batch = files[i:i + self.BATCH_SIZE]
            
            logger.info(f"\nğŸ”„ ë°°ì¹˜ {batch_num}: {len(batch)}ê°œ íŒŒì¼ ì²˜ë¦¬ ì¤‘...")
            
            documents = []
            
            for file_path in batch:
                if not file_path.exists():
                    continue
                
                try:
                    # ë¬¸ì„œ ì²˜ë¦¬
                    docs, _ = processor.process_directory(file_path.parent)
                    documents.extend(docs)
                    
                    # íŒŒì¼ í•´ì‹œ ê³„ì‚° (ìˆ˜ì • ê°ì§€ìš©)
                    file_hash = self._calculate_file_hash(file_path)
                    
                    # ì›ë³¸ ê²½ë¡œ ë³µì› (íŒŒì¼ëª…ì—ì„œ)
                    file_name = file_path.name
                    original_path = file_name.replace('_', '/')
                    
                    # ìƒíƒœ ì—…ë°ì´íŠ¸ (ì „ì²´ ê²½ë¡œë¥¼ í‚¤ë¡œ ì‚¬ìš©)
                    self.state['processed_files'][original_path] = {
                        'modified_time': datetime.now().isoformat(),
                        'file_hash': file_hash,
                        'file_size': file_path.stat().st_size,
                        'chunks': len(docs),
                        'status': 'processed'
                    }
                    
                    self.stats['processed_files'] += 1
                
                except Exception as e:
                    logger.error(f"âŒ ì²˜ë¦¬ ì‹¤íŒ¨ ({file_path.name}): {e}")
                    self.stats['failed_files'] += 1
                    self.stats['errors'].append(f"ì²˜ë¦¬ ì‹¤íŒ¨: {file_path.name}")
            
            # ë²¡í„° DBì— ì¶”ê°€
            if documents:
                result = vector_store.add_documents(documents)
                chunks_added = result['added']
                total_chunks += chunks_added
                self.stats['total_chunks'] += chunks_added
                
                progress = min(i + self.BATCH_SIZE, len(files))
                logger.info(
                    f"âœ… ë°°ì¹˜ {batch_num} ì™„ë£Œ: "
                    f"{progress}/{len(files)} íŒŒì¼, "
                    f"{chunks_added}ê°œ ì²­í¬ ì¶”ê°€"
                )
            
            batch_num += 1
        
        logger.info(f"âœ… ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ: ì´ {total_chunks}ê°œ ì²­í¬ ìƒì„±")
        
        return len(files), total_chunks
    
    def generate_report(self):
        """ìµœì¢… ë¦¬í¬íŠ¸ ìƒì„±"""
        
        self.stats['end_time'] = datetime.now().isoformat()
        
        elapsed = (datetime.fromisoformat(self.stats['end_time']) - 
                  datetime.fromisoformat(self.stats['start_time'])).total_seconds()
        
        report = f"""
{'='*80}
ğŸ“Š ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ ë¦¬í¬íŠ¸
{'='*80}

â±ï¸ ì†Œìš” ì‹œê°„: {elapsed:.2f}ì´ˆ

ğŸ“ˆ ì²˜ë¦¬ í†µê³„:
   âœ“ ìŠ¤ìº” íŒŒì¼: {self.stats['scanned_files']}ê°œ
   âœ¨ ì‹ ê·œ íŒŒì¼: {self.stats['new_files']}ê°œ
   ğŸ“ ìˆ˜ì • íŒŒì¼: {self.stats['modified_files']}ê°œ
   âš ï¸ ì‚­ì œ íŒŒì¼: {self.stats['deleted_files']}ê°œ (ê°ì§€ë§Œ, ë¯¸ì²˜ë¦¬)
   â¬‡ï¸ ë‹¤ìš´ë¡œë“œ: {self.stats['downloaded_files']}ê°œ
   âœ“ ì²˜ë¦¬ ì„±ê³µ: {self.stats['processed_files']}ê°œ
   âœ— ì²˜ë¦¬ ì‹¤íŒ¨: {self.stats['failed_files']}ê°œ
   âœ“ ìƒì„± ì²­í¬: {self.stats['total_chunks']}ê°œ

ğŸ—“ï¸ ì‹¤í–‰ ì‹œê°„:
   ì‹œì‘: {self.stats['start_time']}
   ì¢…ë£Œ: {self.stats['end_time']}

{'='*80}
"""
        
        if self.stats['errors']:
            report += f"\nâš ï¸ ì˜¤ë¥˜ ëª©ë¡:\n"
            for error in self.stats['errors']:
                report += f"   - {error}\n"
        
        if self.stats['deleted_files'] > 0:
            report += f"\nğŸ”’ ë³´ì•ˆ ì£¼ì˜:\n"
            report += f"   ì‚­ì œëœ íŒŒì¼ {self.stats['deleted_files']}ê°œê°€ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
            report += f"   ë°ì´í„° ë¬´ê²°ì„± ë³´í˜¸ë¥¼ ìœ„í•´ ë²¡í„° DBì—ì„œ ì œê±°í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\n"
            report += f"   í•„ìš”ì‹œ ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”.\n"
        
        report += f"\n{'='*80}\n"
        
        logger.info(report)
        
        # ë¦¬í¬íŠ¸ ì €ì¥
        report_file = Settings.LOGS_DIR / f"batch_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
        
        logger.info(f"âœ… ë¦¬í¬íŠ¸ ì €ì¥: {report_file}")
        
        return report
    
    def run(self):
        """ë°°ì¹˜ ì‹¤í–‰ (ì „ì²´ íŒŒì´í”„ë¼ì¸)
        
        ë‹¨ê³„:
        1. íŒŒì¼ ìŠ¤ìº” (ë°œê²¬) - í•˜ìœ„ ë””ë ‰í† ë¦¬ ì¬ê·€ íƒìƒ‰ âœ…
        2. íŒŒì¼ ë¶„ë¥˜ (ì‹ ê·œ, ìˆ˜ì •, ì‚­ì œ)
        3. íŒŒì¼ ë‹¤ìš´ë¡œë“œ
        4. ìƒ˜í”Œ í…ŒìŠ¤íŠ¸
        5. ë°°ì¹˜ ì²˜ë¦¬ (ì‹ ê·œ + ìˆ˜ì •)
        6. ìƒíƒœ ì €ì¥
        7. ë¦¬í¬íŠ¸ ìƒì„±
        """
        
        self.stats['start_time'] = datetime.now().isoformat()
        
        logger.info("\n" + "="*80)
        logger.info("ğŸš€ ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘")
        logger.info("="*80)
        
        try:
            # 1ï¸âƒ£ íŒŒì¼ ìŠ¤ìº” (í•˜ìœ„ ë””ë ‰í† ë¦¬ í¬í•¨)
            all_files = self.scan_files()
            if not all_files:
                logger.warning("ì²˜ë¦¬í•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")
                return
            
            # 2ï¸âƒ£ íŒŒì¼ ë¶„ë¥˜
            new_files, modified_files, deleted_files = self.classify_files(all_files)
            
            # ì‹ ê·œ + ìˆ˜ì • íŒŒì¼ í•©ì¹˜ê¸°
            files_to_process = new_files + modified_files
            
            if not files_to_process:
                logger.info("ì²˜ë¦¬í•  ìƒˆ íŒŒì¼ì´ë‚˜ ìˆ˜ì • íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")
                if deleted_files:
                    logger.warning(f"âš ï¸ ì‚­ì œëœ íŒŒì¼ {len(deleted_files)}ê°œëŠ” ë¯¸ì²˜ë¦¬ (ë³´ì•ˆ)")
                return
            
            # 3ï¸âƒ£ ë‹¤ìš´ë¡œë“œ
            downloaded = self.download_files(files_to_process[:10])  # í…ŒìŠ¤íŠ¸ìš© 10ê°œë§Œ
            if not downloaded:
                logger.error("ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")
                return
            
            # 4ï¸âƒ£ ìƒ˜í”Œ í…ŒìŠ¤íŠ¸
            if not self.test_sample(downloaded):
                logger.error("ìƒ˜í”Œ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨, ì¤‘ë‹¨")
                return
            
            # 5ï¸âƒ£ ë°°ì¹˜ ì²˜ë¦¬
            processed, chunks = self.process_batch(downloaded, files_to_process[:10])
            
            # 6ï¸âƒ£ ìƒíƒœ ì €ì¥
            self.state['last_run'] = datetime.now().isoformat()
            self.state['total_chunks'] = self.stats['total_chunks']
            self._save_state()
            
            # 7ï¸âƒ£ ë¦¬í¬íŠ¸ ìƒì„±
            self.generate_report()
        
        except Exception as e:
            logger.error(f"âŒ ë°°ì¹˜ ì‹¤íŒ¨: {e}", exc_info=True)
            self.stats['errors'].append(f"ë°°ì¹˜ ì‹¤íŒ¨: {e}")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    batch = BatchProcessor()
    batch.run()


if __name__ == "__main__":
    main()
