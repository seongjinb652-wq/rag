import os
import uvicorn
import io
import logging # ì¶”ê°€
from fastapi import FastAPI, HTTPException, UploadFile, File
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
# ì›ë³¸: from langchain_openai import OpenAIEmbeddings, ChatOpenAI
# ìˆ˜ì •ë³¸: ë¡œì»¬ ì„ë² ë”© ì‚¬ìš©ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¶”ê°€
from langchain_community.embeddings import HuggingFaceEmbeddings 
from langchain_openai import ChatOpenAI
from langchain_community.vectorstores import Chroma
from faster_whisper import WhisperModel

# ì„¤ì • íŒŒì¼ ë¡œë“œ (ì¶”ê°€)
from config import Settings
from alias_map import clean_and_refine

# ë¡œê±° ì„¤ì •
logger = logging.getLogger("uvicorn")

# 1. ì´ˆê¸°í™” ë° ì„¤ì •
# ì›ë³¸: DB_PATH = "./chroma_db"
# ì›ë³¸: COLLECTION_NAME = "project_docs"
# ìˆ˜ì •ë³¸: config.pyì˜ ì„¤ì •ì„ ê°•ì œ ì—°ê²° (ë¶ˆì¼ì¹˜ ì‹œ ì—ëŸ¬ ë°œìƒ)
DB_PATH = str(Settings.CHROMA_DB_PATH)
COLLECTION_NAME = Settings.CHROMA_COLLECTION_NAME
EMBEDDING_MODEL_NAME = Settings.EMBEDDING_MODEL

app = FastAPI(title="FS Voice RAG System (large-v3)")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

# RAG ì»´í¬ë„ŒíŠ¸ ë¡œë“œ
# ì›ë³¸: embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
# ìˆ˜ì •ë³¸: 2.3GB DBë¥¼ ë§Œë“  ë¡œì»¬ ëª¨ë¸ê³¼ ë™ì¼í•˜ê²Œ ì„¤ì • (í•µì‹¬!)
embeddings = HuggingFaceEmbeddings(
    model_name=EMBEDDING_MODEL_NAME,
    model_kwargs={'device': 'cpu'}
)

vector_db = Chroma(
    persist_directory=DB_PATH,
    embedding_function=embeddings,
    collection_name=COLLECTION_NAME
)
llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0)

# Whisper Large-v3 ëª¨ë¸ ë¡œë“œ (i7-14700 / 32GB RAM ìµœì í™”)
print("â³ Whisper STT ì—”ì§„(Large-v3) ë¡œë”© ì¤‘... (ì•½ 3GB)")
stt_model = WhisperModel("large-v3", device="cpu", compute_type="int8")

print("âœ… ì—”ì§„ ì¤€ë¹„ ì™„ë£Œ")

# 2. ê³µí†µ ê²€ìƒ‰ ë¡œì§
def perform_rag_search(query: str):
    refined_query = clean_and_refine(query)
    print(f"ğŸ” [ìµœì¢… êµì • ì¿¼ë¦¬]: {refined_query}")
    
    # ì›ë³¸: docs = vector_db.similarity_search(refined_query, k=5)
    # ìˆ˜ì •ë³¸: config.pyì˜ ê²€ìƒ‰ Kê°’ ì ìš©
    docs = vector_db.similarity_search(refined_query, k=Settings.VECTOR_SEARCH_K)
    
    context_list = []
    sources = []
    root_folder_name = "@@@ì¸ë„ë„¤ì‹œì•„PDTì•”ì„¼í„°FS"
    
    for d in docs:
        content = d.page_content
        
        # 1. ë³¸ë¬¸ ì•ˆì— "Source:"ë¼ëŠ” ë‹¨ì–´ê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
        if "Source:" in content:
            lines = content.split('\n')
            source_line = ""
            actual_body = []
            
            for line in lines:
                if line.startswith("Source:"):
                    source_line = line.replace("Source:", "").strip()
                elif line.strip().startswith("---"): 
                    continue
                else:
                    actual_body.append(line)
            
            # ê²½ë¡œ ê°„ì†Œí™” ì²˜ë¦¬
            if source_line:
                source_line = source_line.replace('\\', '/')
                target_root = root_folder_name.replace('\\', '/')
                
                if target_root in source_line:
                    display_path = source_line.split(target_root)[-1].lstrip('/')
                else:
                    display_path = os.path.basename(source_line)
                
                sources.append(display_path)
            
            context_list.append("\n".join(actual_body))
        else:
            # Source ë¬¸êµ¬ê°€ ì—†ëŠ” ê²½ìš° (v3 ë°©ì‹ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ)
            context_list.append(content)
            # ì›ë³¸: raw_src = d.metadata.get("source", "ì•Œ ìˆ˜ ì—†ìŒ")
            # ìˆ˜ì •ë³¸: metadata['source'] í‚¤ í™•ì¸
            raw_src = d.metadata.get("source") or d.metadata.get("file_path") or "ì•Œ ìˆ˜ ì—†ìŒ"
            sources.append(os.path.basename(raw_src))
    
    sources = sorted(list(set([s for s in sources if s]))) 
    context = "\n\n".join(context_list)
    
    prompt = f"ë‹¤ìŒ ë¬¸ë§¥ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ì •í™•íˆ ë‹µí•˜ì„¸ìš”:\n\n{context}\n\nì§ˆë¬¸: {refined_query}"
    
    try:
        response = llm.invoke(prompt)
        answer = response.content
    except Exception as e:
        logger.error(f"LLM í˜¸ì¶œ ì—ëŸ¬: {e}")
        answer = "ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ì— ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

    return {
        "original_text": query,
        "refined_query": refined_query,
        "answer": answer,
        "sources": sources
    }

# 3. API ì—”ë“œí¬ì¸íŠ¸
class ChatRequest(BaseModel):
    # ì›ë³¸: message: str
    # ìˆ˜ì •ë³¸: curl í…ŒìŠ¤íŠ¸ ì‹œ 'text'ë¡œ ë³´ë‚´ì…¨ìœ¼ë¯€ë¡œ í˜¸í™˜ì„± ìœ„í•´ message ë˜ëŠ” text ì§€ì›
    message: str = None
    text: str = None

@app.post("/chat")
@app.post("/query") # ì¶”ê°€: curl í…ŒìŠ¤íŠ¸ ì‹œ ì‚¬ìš©í•œ /query ì—”ë“œí¬ì¸íŠ¸ ì§€ì›
async def chat_text(request: ChatRequest):
    query_text = request.message or request.text
    if not query_text:
        raise HTTPException(status_code=400, detail="message or text is required")
    return perform_rag_search(query_text)

@app.post("/voice")
async def chat_voice(file: UploadFile = File(...)):
    try:
        audio_bytes = await file.read()
        audio_file = io.BytesIO(audio_bytes)
        
        segments, info = stt_model.transcribe(audio_file, beam_size=5, language="ko")
        voice_text = " ".join([segment.text for segment in segments])
        
        print(f"ğŸ™ï¸ [STT ì¸ì‹]: {voice_text}")
        return perform_rag_search(voice_text)
    except Exception as e:
        print(f"âŒ ì—ëŸ¬ ë°œìƒ: {e}")
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000, workers=1)
