import os
import json
import shutil
import time
import logging
import re
from datetime import datetime
# [2026-01-31 ì„±ì§„ ì¶”ê°€ ì •ì˜] ë¡œì»¬ ì„ë² ë”©ìš© ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¶”ê°€
from langchain_huggingface import HuggingFaceEmbeddings
# [ê¸°ì¡´ ìœ ì§€]
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_text_splitters import RecursiveCharacterTextSplitter
from config import Settings  # ëª¨ë“  ìƒìˆ˜ëŠ” ì—¬ê¸°ì„œ ì°¸ì¡°

# 1. ì—ëŸ¬ ë¡œê·¸ ì„¤ì •
log_file_path = Settings.LOGS_DIR / f"loader_error_{datetime.now().strftime('%Y%m%d')}.log"
logging.basicConfig(
    filename=log_file_path,
    level=logging.ERROR,
    format='%(asctime)s - %(levelname)s - %(message)s',
    encoding=Settings.ENCODING  # [2026-01-31 ì„±ì§„ ë³€ìˆ˜ ì²˜ë¦¬]
)

def get_db_status(vector_db):
    try:
        return vector_db._collection.count()
    except Exception:
        return 0

def process_and_save():
    db_path = str(Settings.CHROMA_DB_PATH)
    state_file = Settings.BATCH_STATE_FILE
    input_dir = Settings.DATA_DIR / "text_converted"
    
    # =========================================================
    # [2026-01-31 ì„±ì§„ ì£¼ì„ ë³´ì¡´] ê¸°ì¡´ OpenAIEmbeddings ì„¤ì •
    # =========================================================
    # embeddings = OpenAIEmbeddings(model=Settings.EMBEDDING_MODEL)
    
    # [2026-01-31 ì„±ì§„ ì¶”ê°€ ì •ì˜] v5: ArtistSum ë²¤ì¹˜ë§ˆí¬ìš© ë¡œì»¬ ëª¨ë¸ (ìƒìˆ˜ ë³€ìˆ˜í™” ì™„ë£Œ)
    print(f"ğŸ”„ ë¡œì»¬ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘: {Settings.EMBEDDING_MODEL}...")
    embeddings = HuggingFaceEmbeddings(
        model_name=Settings.EMBEDDING_MODEL,
        model_kwargs=Settings.EMBEDDING_KWARGS,
        encode_kwargs=Settings.ENCODE_KWARGS
    )
    # =========================================================
    
    # [ì´ˆê¸°í™” ì ˆì°¨]
    if Settings.RESET_DB:
        now = datetime.now().strftime("%H:%M:%S")
        print(f"[{now}] âš ï¸ [v3 ì´ˆê¸°í™” ëª¨ë“œ] ê¸°ì¡´ ë°ì´í„°ë¥¼ ì‚­ì œí•©ë‹ˆë‹¤. (ArtistSum êµ¬ì¶•)")
        if os.path.exists(db_path):
            shutil.rmtree(db_path)
        if os.path.exists(state_file):
            os.remove(state_file)
        print(f"[{now}] ğŸ—‘ï¸  DB ë° ìƒíƒœ íŒŒì¼ ì‚­ì œ ì™„ë£Œ.")

    # 2. ë²¡í„° DB ì—°ê²°
    vector_db = Chroma(
        persist_directory=db_path,
        embedding_function=embeddings,
        collection_name=Settings.CHROMA_COLLECTION_NAME
    )

    # 3. ìƒíƒœ í™•ì¸ (v4 ì´ì–´ë„£ê¸°ìš©)
    initial_count = get_db_status(vector_db)
    processed_files = set()
    if not Settings.RESET_DB and os.path.exists(state_file):
        with open(state_file, "r", encoding=Settings.ENCODING) as f:
            processed_files = set(json.load(f))

    # 4. ëŒ€ìƒ íŒŒì¼ ëª©ë¡ ì¶”ì¶œ
    all_files = [f for f in os.listdir(input_dir) if f.endswith(".txt")]
    files_to_process = [f for f in all_files if f not in processed_files]
    total_files = len(files_to_process)
    
    print(f"\nğŸ“Š [DB í˜„í™©] ê¸°ì¡´ ë°ì´í„°: {initial_count}ê±´")
    print(f"ğŸš€ [ì‘ì—… ì‹œì‘] ArtistSum ì²˜ë¦¬ ëŒ€ìƒ: {total_files}ê°œ íŒŒì¼\n")

    # ìŠ¤í”Œë¦¬í„° ì„¤ì •
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=Settings.CHUNK_SIZE,
        chunk_overlap=Settings.CHUNK_OVERLAP,
        separators=["\n\n", "\n", " ", ""]
    )

    # 5. ë©”ì¸ ì²˜ë¦¬ ë£¨í”„
    total_added_chunks = 0
    
    for idx, file_name in enumerate(files_to_process, 1):
        file_path = os.path.join(input_dir, file_name)
        now_time = datetime.now().strftime("%H:%M:%S")
        
        try:
            with open(file_path, "r", encoding=Settings.ENCODING, errors=Settings.FILE_ERRORS_STRATEGY) as f:
                full_content = f.read()
            
            lines = full_content.split('\n')
            
            # ì¶œì²˜ ë³µì› ë° ë³¸ë¬¸ ì •ì œ
            if lines and lines[0].startswith("Source:"):
                full_source_path = lines[0].replace("Source:", "").strip()
                original_name = full_source_path.replace('\\', '/').split('/')[-1]
                content_body = "\n".join(lines[2:]).strip()
            else:
                original_name = file_name.rsplit('_', 1)[0]
                content_body = full_content

            # ì²­í¬ ìƒì„±
            chunks = text_splitter.split_text(content_body)
            num_chunks = len(chunks)

            # =========================================================
            # [2026-01-31 ì„±ì§„ ì£¼ì„ ë³´ì¡´] ê¸°ì¡´ ë‹¨ì¼ ë©”íƒ€ë°ì´í„° ì„¤ì •
            # metadatas = [{Settings.META_SOURCE_KEY: original_name} for _ in range(num_chunks)]
            
            # [2026-01-31 ì„±ì§„ ì¶”ê°€ ì •ì˜] í™•ì¥ ë©”íƒ€ë°ì´í„° ë° ë³€ìˆ˜ ì¶”ì¶œ (ArtistSum ì „ìš©)
            batch_metadatas = []
            
            # [ë³€ìˆ˜ ì²˜ë¦¬] ì‹¤ì œ ë¬¸ì„œ ê¸°ì¤€ ì—°ë„ ì¶”ì¶œ (ì¶”ì¶œ ë²”ìœ„ ìƒìˆ˜í™”)
            doc_year = "Unknown"
            year_match = re.search(r'(19|20)\d{2}', file_name + content_body[:Settings.META_EXTRACT_LIMIT])
            if year_match:
                doc_year = year_match.group()

            for _ in range(num_chunks):
                meta = {
                    Settings.META_SOURCE_KEY: original_name,
                    Settings.META_YEAR_KEY: doc_year,
                    Settings.META_PROJECT_NAME: "ArtistSum",
                    Settings.META_DOC_TYPE: "ë¯¸ë¶„ë¥˜",
                    Settings.META_INDUSTRY_KEY: None,
                    Settings.META_AUTHOR_KEY: None,
                    Settings.META_TOC_KEY: None,
                    Settings.META_SECTION_KEY: None,
                    Settings.META_ANCHOR_KEY: None,
                    Settings.META_PAGE_KEY: None
                }
                batch_metadatas.append(meta)
            # =========================================================

            # ---------------------------------------------------------
            # [2026-01-31 ì„±ì§„ ì£¼ì„ ë³´ì¡´] ì›ë³¸ ë¶„í•  ì ì¬ ë£¨í”„ ë° ì—ëŸ¬ ì²˜ë¦¬
            # ---------------------------------------------------------
            """
            chunk_batch_size = 100 
            for i in range(0, num_chunks, chunk_batch_size):
                # ... ê¸°ì¡´ ë¡œì§ ë³´ì¡´ (ì¤‘ëµ) ...
            """

            # [2026-01-31 ì„±ì§„ ì¶”ê°€ ì •ì˜] BGE-M3 ë¡œì»¬ ì „ìš© ê³ ì† ì ì¬
            vector_db.add_texts(texts=chunks, metadatas=batch_metadatas)
            # ---------------------------------------------------------

            # ìƒíƒœ ì—…ë°ì´íŠ¸
            total_added_chunks += num_chunks
            processed_files.add(file_name)
            with open(state_file, "w", encoding=Settings.ENCODING) as f:
                json.dump(list(processed_files), f, ensure_ascii=False, indent=4)
            
            # ë””ë²„ê·¸ìš© ì¶œë ¥ ì œì–´ (ìƒìˆ˜ ì°¸ì¡°)
            if num_chunks >= Settings.LARGE_FILE_THRESHOLD: 
                print(f"\n[{now_time}] ğŸ˜ [ëŒ€í˜•] ({idx}/{total_files}) {original_name} (ì²­í¬: {num_chunks}ê°œ)")
            
            if idx % Settings.DISPLAY_INTERVAL == 0:
                print(f"\n[{now_time}] ğŸ“¦ [ë°°ì¹˜] {idx}/{total_files} ì™„ë£Œ (ëˆ„ì  ì²­í¬: {total_added_chunks})")
            else:
                print(f"\r[{now_time}] ({idx}/{total_files}) ì²˜ë¦¬ ì¤‘: {original_name[:25]}...", end="")

        except Exception as e:
            err_msg = f"ì‹¤íŒ¨: {file_name} | ì´ìœ : {str(e)}"
            print(f"\n[{now_time}] âŒ {err_msg}")
            logging.error(err_msg)

    # 6. ìµœì¢… ê²°ê³¼
    final_count = get_db_status(vector_db)
    print("\n\n" + "="*60)
    print(f"ğŸ ArtistSum ëª¨ë“  ë°ì´í„° ì ì¬ ì™„ë£Œ (BGE-M3 768dim)")
    print(f"ğŸ“ˆ DB ì²­í¬ ë³€í™”: {initial_count} -> {final_count} (ì¦ë¶„: {total_added_chunks})")
    print(f"ğŸ“„ ì—ëŸ¬ ë¡œê·¸: {log_file_path.name}")
    print("="*60)

if __name__ == "__main__":
    process_and_save()
