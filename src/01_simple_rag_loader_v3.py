# (ë‹¨ë½ë³´ì¡´ + í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜í˜• + ë©”ëª¨ë¦¬ ì´ˆê¸°í™” + .env ë¡œë“œ)
import os
import shutil
import logging
from pathlib import Path
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_text_splitters import RecursiveCharacterTextSplitter
# from langchain_chroma import Chroma
from dotenv import load_dotenv # .env ë¡œë“œ í•¨ìˆ˜

# .env íŒŒì¼ ë¡œë“œ
load_dotenv() 

# ì´ì œ os.getenvë¥¼ í†µí•´ ì•ˆì „í•˜ê²Œ ê°€ì ¸ì˜µë‹ˆë‹¤.
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

# í™•ì¸ìš© (í‚¤ì˜ ì• 5ìë¦¬ë§Œ ì¶œë ¥í•´ì„œ ì˜ ê°€ì ¸ì™”ëŠ”ì§€ ì²´í¬)
if OPENAI_API_KEY:
    print(f"ğŸ”‘ API KEY ë¡œë“œ ì„±ê³µ: {OPENAI_API_KEY[:5]}*****")
else:
    print("âŒ .env íŒŒì¼ì—ì„œ OPENAI_API_KEYë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")


# ê²½ë¡œ ë° ì„¤ì •
TXT_DIR = Path(r"C:/Users/USER/rag/src/data/text_converted")
DB_PATH = r"C:/Users/USER/rag/src/data/chroma_db"
COLLECTION_NAME = "indonesia_pdt_docs"

def initialize_and_load():
    # 1. DB ì´ˆê¸°í™”
    if os.path.exists(DB_PATH):
        print(f"ğŸ—‘ï¸ ê¸°ì¡´ DB ì‚­ì œ ë° ì´ˆê¸°í™”: {DB_PATH}")
        shutil.rmtree(DB_PATH)

    # 2. ëª¨ë¸ ë° ìŠ¤í”Œë¦¬í„° ì„¤ì •
    embeddings = OpenAIEmbeddings(model="text-embedding-3-small") # ê°€ì„±ë¹„ ì¢‹ì€ ìµœì‹  ëª¨ë¸
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000, 
        chunk_overlap=150,
        separators=["\n\n", "\n", " ", ""]
    )

    # 3. íŒŒì¼ ëª©ë¡
    all_files = list(TXT_DIR.glob("*.txt"))
    print(f"ğŸš€ ì´ {len(all_files)}ê°œ íŒŒì¼ DB ì ì¬ ì‹œì‘...")

    # ì´ˆê¸° DB ìƒì„±
    vector_db = None

    # 4. ë°°ì¹˜ ì²˜ë¦¬ (ë©”ëª¨ë¦¬ íš¨ìœ¨í™”)
    batch_size = 5 
    for i in range(0, len(all_files), batch_size):
        batch_files = all_files[i : i + batch_size]
        texts = []
        metadatas = []

        for file_path in batch_files:
            try:
                with open(file_path, "r", encoding="utf-8") as f:
                    content = f.read()
                    chunks = text_splitter.split_text(content)
                    for chunk in chunks:
                        texts.append(chunk)
                        metadatas.append({"source": file_path.name})
            except Exception as e:
                print(f"âŒ ì˜¤ë¥˜ ({file_path.name}): {e}")

        # DBì— ë°ì´í„° ì¶”ê°€
        if texts:
            if vector_db is None:
                vector_db = Chroma.from_texts(
                    texts=texts,
                    embedding=embeddings,
                    metadatas=metadatas,
                    persist_directory=DB_PATH,
                    collection_name=COLLECTION_NAME
                )
            else:
                vector_db.add_texts(texts=texts, metadatas=metadatas)
                
            vector_db.persist()  # 0.4.x ë²„ì „ì—ì„œ ë°ì´í„°ë¥¼ ë””ìŠ¤í¬ì— ì¦‰ì‹œ ì“°ë„ë¡ ê°•ì œí•¨
            print(f"âœ… ë°°ì¹˜ ì™„ë£Œ: {min(i + batch_size, len(all_files))} / {len(all_files)}")

    print(f"ğŸ DB êµ¬ì¶• ì™„ë£Œ! ìœ„ì¹˜: {DB_PATH}")

if __name__ == "__main__":
    initialize_and_load()
