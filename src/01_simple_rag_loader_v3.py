# (ë‹¨ë½ë³´ì¡´ + í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜í˜• + ë©”ëª¨ë¦¬ ì´ˆê¸°í™” + .env ë¡œë“œ)
import os
import shutil
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import TextLoader
from config import Settings  # ì¤‘ì•™ ì„¤ì • ì°¸ì¡°

def process_and_save():
    # 1. DB ë° ëª¨ë¸ ì„¤ì • (ê¸°ì¡´ ê°’ ì£¼ì„ ë³´ì¡´)
    # DB_PATH = r"C:/Users/USER/rag/src/data/chroma_db"
    db_path = str(Settings.CHROMA_DB_PATH)
    # COLLECTION_NAME = "indonesia_pdt_docs"
    collection_name = Settings.CHROMA_COLLECTION_NAME
    # embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
    embeddings = OpenAIEmbeddings(model=Settings.EMBEDDING_MODEL)

    # [v3 ì›ì¹™: ì´ˆê¸°í™”] ê¸°ì¡´ DB í´ë”ê°€ ìˆë‹¤ë©´ ì‚­ì œí•˜ì—¬ ê¹¨ë—í•˜ê²Œ ì‹œì‘
    if os.path.exists(db_path):
        print(f"ğŸ—‘ï¸ ê¸°ì¡´ DB ì´ˆê¸°í™” ì¤‘... ({db_path})")
        shutil.rmtree(db_path)

    # 2. í…ìŠ¤íŠ¸ ë¶„í•  ì„¤ì • (ê¸°ì¡´ ê°’ ì£¼ì„ ë³´ì¡´)
    # text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=Settings.CHUNK_SIZE,
        chunk_overlap=Settings.CHUNK_OVERLAP
    )

    # 3. ì‹ ê·œ ë²¡í„° DB ìƒì„±
    vector_db = Chroma(
        persist_directory=db_path,
        embedding_function=embeddings,
        collection_name=collection_name
    )

    # 4. ì‘ì—… ëŒ€ìƒ íŒŒì¼ ëª©ë¡
    input_dir = Settings.DATA_DIR / "text_converted"
    all_files = [f for f in os.listdir(input_dir) if f.endswith(".txt")]
    
    print(f"ğŸš€ ì´ {len(all_files)}ê°œ íŒŒì¼ ì ì¬ ì‹œì‘ (ì´ˆê¸°í™” v3 ëª¨ë“œ)")

    for file_name in all_files:
        file_path = os.path.join(input_dir, file_name)
        try:
            loader = TextLoader(file_path, encoding='utf-8')
            raw_docs = loader.load()
            
            # [ì§€ì‹œì‚¬í•­ ë°˜ì˜] ë³¸ë¬¸ ì²« ì¤„ì—ì„œ ì›ë³¸ íŒŒì¼ëª… ì¶”ì¶œ ë° ë³¸ë¬¸ ì •ì œ
            for doc in raw_docs:
                lines = doc.page_content.split('\n')
                
                if lines and lines[0].startswith("Source:"):
                    # 1) ì›ë³¸ ê²½ë¡œ ì¶”ì¶œ ë° OS í†µí•© ëŒ€ì‘ (ìœˆë„ìš°/ë§¥)
                    full_source_path = lines[0].replace("Source:", "").strip()
                    unified_path = full_source_path.replace('\\', '/')
                    # 2) íŒŒì¼ëª…(í™•ì¥ì í¬í•¨)ë§Œ ì¶”ì¶œ
                    original_name = unified_path.split('/')[-1]
                    
                    # 3) ë³¸ë¬¸ ì •ì œ: Sourceì¤„ê³¼ êµ¬ë¶„ì„  ì œê±° (2í–‰ë¶€í„° ë³¸ë¬¸ ì‹œì‘)
                    doc.page_content = "\n".join(lines[2:]).strip()
                else:
                    # ì˜ˆì™¸ ë°œìƒ ì‹œ txt íŒŒì¼ëª…ì—ì„œ í•´ì‹œ ì œê±°í•˜ì—¬ ì‚¬ìš©
                    original_name = file_name.rsplit('_', 1)[0]
                
                # 4) ë©”íƒ€ë°ì´í„°ì— ì›ë³¸ íŒŒì¼ëª…(.pdf ë“±) ê¸°ë¡
                # doc.metadata["source"] = file_path
                doc.metadata[Settings.META_SOURCE_KEY] = original_name
            
            # ì²­í¬ ë¶„í•  ë° ì ì¬
            final_chunks = text_splitter.split_documents(raw_docs)
            vector_db.add_documents(final_chunks)
            print(f"âœ… ì ì¬ ì™„ë£Œ: {original_name}")

        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜ ë°œìƒ ({file_name}): {e}")

if __name__ == "__main__":
    process_and_save()
