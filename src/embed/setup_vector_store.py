#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ë²¡í„° ì„ë² ë”© + Chroma DB ì €ì¥
ëª©í‘œ: í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜ í›„ Chroma DBì— ì €ì¥

ê¸°ëŠ¥:
- í•œêµ­ì–´ ìµœì í™” ì„ë² ë”© (Ko-SBERT)
- Chroma DB ì´ˆê¸°í™”
- ë¬¸ì„œ ì¶”ê°€ ë° ê²€ìƒ‰
- ë²¡í„° ìœ ì‚¬ë„ ê³„ì‚°

ì‹¤í–‰: python setup_vector_store.py
"""

import os
import sys
from pathlib import Path
from typing import List, Dict, Tuple
import logging
import json

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ë¶€ëª¨ í´ë”ì—ì„œ config ì„í¬íŠ¸
PROJECT_ROOT = Path(__file__).parent.parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

import importlib.util
spec = importlib.util.spec_from_file_location("config", config_file)
config_module = importlib.util.module_from_spec(spec)
spec.loader.exec_module(config_module)
Settings = config_module.Settings

class VectorStore:
    """ë²¡í„° ì €ì¥ì†Œ - Chroma DB"""
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        import chromadb
        from sentence_transformers import SentenceTransformer
        
        logger.info("ğŸ”§ ë²¡í„° ì €ì¥ì†Œ ì´ˆê¸°í™” ì¤‘...")
        
        # Chroma DB ì´ˆê¸°í™”
        self.db_path = str(Settings.CHROMA_DB_PATH)
        Settings.CHROMA_DB_PATH.mkdir(parents=True, exist_ok=True)
        
        self.client = chromadb.PersistentClient(path=self.db_path)
        
        # ê¸°ì¡´ ì»¬ë ‰ì…˜ì´ ìˆìœ¼ë©´ ì‚­ì œ í›„ ì¬ìƒì„± (í…ŒìŠ¤íŠ¸ ëª¨ë“œ)
        try:
            self.client.delete_collection(name=Settings.CHROMA_COLLECTION_NAME)
            logger.info(f"ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚­ì œ: {Settings.CHROMA_COLLECTION_NAME}")
        except:
            pass
        
        # ìƒˆ ì»¬ë ‰ì…˜ ìƒì„±
        self.collection = self.client.get_or_create_collection(
            name=Settings.CHROMA_COLLECTION_NAME,
            metadata={"hnsw:space": "cosine"}
        )
        
        logger.info(f"âœ… Chroma DB ìƒì„±: {self.db_path}")
        
        # ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
        logger.info("ğŸ¤– ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘...")
        self.model = SentenceTransformer(Settings.EMBEDDING_MODEL)
        logger.info(f"âœ… ëª¨ë¸ ë¡œë“œ: {Settings.EMBEDDING_MODEL}")
        
        self.doc_count = 0
    
    def add_documents(self, documents: List[Dict]) -> Dict:
        """ë¬¸ì„œë¥¼ ë²¡í„°ë¡œ ë³€í™˜ í›„ DBì— ì¶”ê°€"""
        
        logger.info(f"ğŸ“ {len(documents)}ê°œ ë¬¸ì„œ ì¶”ê°€ ì‹œì‘...")
        
        if not documents:
            logger.warning("ì¶”ê°€í•  ë¬¸ì„œ ì—†ìŒ")
            return {'added': 0, 'skipped': 0}
        
        # ë°°ì¹˜ ì²˜ë¦¬ (100ê°œì”©)
        batch_size = 100
        total_added = 0
        total_skipped = 0
        
        for i in range(0, len(documents), batch_size):
            batch = documents[i:i + batch_size]
            
            ids = []
            texts = []
            embeddings = []
            metadatas = []
            
            for idx, doc in enumerate(batch):
                text = doc.get('text', '')
                source = doc.get('source', 'unknown')
                
                if not text.strip():
                    total_skipped += 1
                    continue
                
                # ì„ë² ë”© ìƒì„±
                embedding = self.model.encode(text, convert_to_numpy=True)
                
                doc_id = f"doc_{self.doc_count}_{idx}"
                ids.append(doc_id)
                texts.append(text)
                embeddings.append(embedding.tolist())
                metadatas.append({
                    'source': source,
                    'length': len(text)
                })
                
                total_added += 1
            
            # ë°°ì¹˜ ì¶”ê°€
            if ids:
                self.collection.upsert(
                    ids=ids,
                    embeddings=embeddings,
                    documents=texts,
                    metadatas=metadatas
                )
                
                progress = min(i + batch_size, len(documents))
                logger.info(f"   ì§„í–‰: {progress}/{len(documents)} ë¬¸ì„œ")
            
            self.doc_count += batch_size
        
        logger.info(f"âœ… ì¶”ê°€ ì™„ë£Œ: {total_added}ê°œ (ìŠ¤í‚µ: {total_skipped}ê°œ)")
        
        return {
            'added': total_added,
            'skipped': total_skipped,
            'total_docs': self.collection.count()
        }
    
    def search(self, query: str, n_results: int = 5) -> List[Dict]:
        """ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰"""
        
        logger.info(f"ğŸ” ê²€ìƒ‰: '{query}'")
        
        try:
            # ì¿¼ë¦¬ ì„ë² ë”©
            query_embedding = self.model.encode(query, convert_to_numpy=True)
            
            # ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰
            results = self.collection.query(
                query_embeddings=[query_embedding.tolist()],
                n_results=n_results
            )
            
            # ê²°ê³¼ ì •ë ¬
            documents = []
            if results['documents'] and len(results['documents']) > 0:
                for i, doc in enumerate(results['documents'][0]):
                    distance = results['distances'][0][i] if results['distances'] else 0
                    similarity = 1 - distance  # cosine ê±°ë¦¬ë¥¼ ìœ ì‚¬ë„ë¡œ ë³€í™˜
                    
                    documents.append({
                        'text': doc,
                        'similarity': round(similarity, 4),
                        'metadata': results['metadatas'][0][i] if results['metadatas'] else {}
                    })
            
            logger.info(f"âœ… ê²€ìƒ‰ ì™„ë£Œ: {len(documents)}ê°œ ê²°ê³¼")
            
            return documents
        
        except Exception as e:
            logger.error(f"âŒ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def get_stats(self) -> Dict:
        """ì €ì¥ì†Œ í†µê³„"""
        
        count = self.collection.count()
        
        stats = {
            'db_path': self.db_path,
            'collection_name': Settings.CHROMA_COLLECTION_NAME,
            'total_documents': count,
            'model': Settings.EMBEDDING_MODEL,
            'embedding_dimension': Settings.EMBEDDING_DIMENSION
        }
        
        return stats
    
    def save_stats(self, file_path: Path = None):
        """í†µê³„ ì €ì¥"""
        
        if file_path is None:
            file_path = Settings.DATA_DIR / 'vector_stats.json'
        
        stats = self.get_stats()
        
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(stats, f, ensure_ascii=False, indent=2)
        
        logger.info(f"âœ… í†µê³„ ì €ì¥: {file_path}")
        
        return stats


def main():
    """í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    
    print("\n" + "="*80)
    print("ğŸš€ ë²¡í„° ì €ì¥ì†Œ ì´ˆê¸°í™” ë° í…ŒìŠ¤íŠ¸")
    print("="*80)
    
    # ë²¡í„° ì €ì¥ì†Œ ìƒì„±
    store = VectorStore()
    
    # í…ŒìŠ¤íŠ¸ ë¬¸ì„œ
    test_documents = [
        {
            'text': 'í•œêµ­ì˜ ìˆ˜ë„ëŠ” ì„œìš¸ì…ë‹ˆë‹¤.',
            'source': 'test_1'
        },
        {
            'text': 'ì„œìš¸ì€ í•œë°˜ë„ ì¤‘ì•™ì— ìœ„ì¹˜í•œ ëŒ€í•œë¯¼êµ­ì˜ ì •ì¹˜, ê²½ì œ, ë¬¸í™”ì˜ ì¤‘ì‹¬ì§€ì…ë‹ˆë‹¤.',
            'source': 'test_2'
        },
        {
            'text': 'ë‚¨ì‚°íƒ€ì›ŒëŠ” ì„œìš¸ì˜ ìœ ëª…í•œ ëœë“œë§ˆí¬ì…ë‹ˆë‹¤.',
            'source': 'test_3'
        },
        {
            'text': 'í•œê°•ì€ ì„œìš¸ì„ ê°€ë¡œì§ˆëŸ¬ íë¥´ëŠ” ì¤‘ìš”í•œ ê°•ì…ë‹ˆë‹¤.',
            'source': 'test_4'
        },
        {
            'text': 'Pythonì€ ì¸ê¸° ìˆëŠ” í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì…ë‹ˆë‹¤.',
            'source': 'test_5'
        },
    ]
    
    # ë¬¸ì„œ ì¶”ê°€
    print("\n1ï¸âƒ£ ë¬¸ì„œ ì¶”ê°€")
    print("-" * 80)
    result = store.add_documents(test_documents)
    print(f"ì¶”ê°€ë¨: {result['added']}, ìŠ¤í‚µ: {result['skipped']}")
    
    # í†µê³„
    print("\n2ï¸âƒ£ ì €ì¥ì†Œ í†µê³„")
    print("-" * 80)
    stats = store.get_stats()
    for key, value in stats.items():
        print(f"{key}: {value}")
    
    # ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
    print("\n3ï¸âƒ£ ìœ ì‚¬ë„ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸")
    print("-" * 80)
    
    queries = [
        'ì„œìš¸ì— ëŒ€í•´ ì•Œë ¤ì¤˜',
        'Pythonì´ë€ ë¬´ì—‡ì¸ê°€?',
        'í•œêµ­ì˜ ì£¼ìš” ë„ì‹œ'
    ]
    
    for query in queries:
        print(f"\nğŸ“Œ ì¿¼ë¦¬: '{query}'")
        results = store.search(query, n_results=3)
        
        for i, result in enumerate(results, 1):
            print(f"   {i}. [{result['similarity']}] {result['text'][:50]}...")
    
    # í†µê³„ ì €ì¥
    print("\n4ï¸âƒ£ í†µê³„ ì €ì¥")
    print("-" * 80)
    store.save_stats()
    
    print("\n" + "="*80)
    print("âœ… ë²¡í„° ì €ì¥ì†Œ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print("="*80 + "\n")


if __name__ == "__main__":
    main()
