import os
from dotenv import load_dotenv
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import Chroma
from langchain.chains import RetrievalQA

# .env ë¡œë“œ
load_dotenv()

# ê²½ë¡œ ì„¤ì •
DB_PATH = r"C:/Users/USER/rag/src/data/chroma_db"
COLLECTION_NAME = "indonesia_pdt_docs"

def test_stable_rag(query):
    # 1. ì„ë² ë”© (ì–´ì œ ì ì¬í•œ ëª¨ë¸ ê·¸ëŒ€ë¡œ ì‚¬ìš©)
    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

    try:
        # 2. DB ì—°ê²° (ì•ˆì •ëœ community ë²„ì „ì˜ Chroma ì‚¬ìš©)
        vector_db = Chroma(
            persist_directory=DB_PATH,
            embedding_function=embeddings,
            collection_name=COLLECTION_NAME
        )

        # 3. ëª¨ë¸ ì„¤ì • (GPT-4o)
        llm = ChatOpenAI(model_name="gpt-4o", temperature=0)

        # 4. RetrievalQA ì²´ì¸ (ê°€ì¥ ì§ê´€ì ì´ê³  ASR ì—°ê²° ì‹œ ê°€ê³µí•˜ê¸° í¸í•¨)
        qa = RetrievalQA.from_chain_type(
            llm=llm,
            chain_type="stuff",
            retriever=vector_db.as_retriever(search_kwargs={"k": 5}),
            return_source_documents=True
        )

        print(f"\nğŸ™‹ ì§ˆë¬¸: {query}")
        print("-" * 50)
        
        result = qa.invoke({"query": query})

        print(f"ğŸ¤– ë‹µë³€:\n{result['result']}")
        print("-" * 50)
        
        # ì†ŒìŠ¤ í™•ì¸
        sources = set([doc.metadata['source'] for doc in result['source_documents']])
        print(f"ğŸ“š ì°¸ê³ ë¬¸í—Œ: {', '.join(sources)}")

    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")

if __name__ == "__main__":
    test_stable_rag("ì¸ë„ë„¤ì‹œì•„ PDT ì‚¬ì—…ì˜ ë¦¬ìŠ¤í¬ê°€ ë­ì•¼? ì‚¬ì—…ì„± í‰ê°€ ë³´ê³ ì„œ ë‚´ìš©ì„ ì¤‘ì‹¬ìœ¼ë¡œ ì•Œë ¤ì¤˜.")
