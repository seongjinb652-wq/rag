import os
from dotenv import load_dotenv
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_chroma import Chroma
from langchain.chains import RetrievalQA

# .env ë¡œë“œ
load_dotenv()

# ì„¤ì •
DB_PATH = r"C:/Users/USER/rag/src/data/chroma_db"
COLLECTION_NAME = "indonesia_pdt_docs"

def test_rag_query(query):
    # 1. ì„ë² ë”© ë° DB ì—°ê²°
    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
    vector_db = Chroma(
        persist_directory=DB_PATH,
        embedding_function=embeddings,
        collection_name=COLLECTION_NAME
    )

    # 2. ëª¨ë¸ ì„¤ì • (ë‹µë³€ìš© GPT-4o)
    llm = ChatOpenAI(model_name="gpt-4o", temperature=0)

    # 3. RAG ì²´ì¸ êµ¬ì¶•
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=vector_db.as_retriever(search_kwargs={"k": 5}), # ê´€ë ¨ ë¬¸ì„œ 5ê°œ ì°¸ì¡°
        return_source_documents=True
    )

    # 4. ì§ˆë¬¸ ì‹¤í–‰
    print(f"\nğŸ™‹ ì§ˆë¬¸: {query}")
    print("-" * 50)
    result = qa_chain.invoke({"query": query})

    print(f"ğŸ¤– ë‹µë³€:\n{result['result']}")
    print("-" * 50)
    
    # ì°¸ì¡°ëœ ì†ŒìŠ¤ íŒŒì¼ í™•ì¸
    print("ğŸ“š ì°¸ê³ í•œ ë¬¸ì„œ ëª©ë¡:")
    sources = set([doc.metadata['source'] for doc in result['source_documents']])
    for src in sources:
        print(f"- {src}")

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì§ˆë¬¸
    user_query = "ì¸ë„ë„¤ì‹œì•„ PDT ì‚¬ì—…ì˜ ë¦¬ìŠ¤í¬ê°€ ë­ì•¼? ì‚¬ì—…ì„± í‰ê°€ ë³´ê³ ì„œ ë‚´ìš©ì„ ì¤‘ì‹¬ìœ¼ë¡œ ì•Œë ¤ì¤˜."
    test_rag_query(user_query)
