#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import logging
import hashlib
from pathlib import Path
from datetime import datetime
from typing import List, Dict

# ì£¼ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬ (í•„ìš” ì‹œ ì„¤ì¹˜: pip install chromadb sentence-transformers PyPDF2 python-docx python-pptx)
import chromadb
from sentence_transformers import SentenceTransformer
from PyPDF2 import PdfReader
from docx import Document
from pptx import Presentation

# 1. ì„¤ì • (Settings í´ë˜ìŠ¤ ëŒ€ì‹  ì§ì ‘ ì„ ì–¸í•˜ì—¬ ë‹¨ìˆœí™”)
TARGET_DIR = Path(r"C:/Users/USER/Downloads/@@@ì¸ë„ë„¤ì‹œì•„PDTì•”ì„¼í„°FS")
DB_PATH = Path(r"C:/Users/USER/rag/src/data/chroma_db")
COLLECTION_NAME = "indonesia_pdt_docs"
EMBED_MODEL_NAME = "snunlp/KR-SBERT-V4-KNOWEE" # í•œêµ­ì–´ ì„±ëŠ¥ì´ ìš°ìˆ˜í•œ ëª¨ë¸ ì¶”ì²œ
CHUNK_SIZE = 600
CHUNK_OVERLAP = 100

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class RobustRAGLoader:
    def __init__(self):
        logger.info("ğŸš€ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
        # DB ì´ˆê¸°í™”
        self.client = chromadb.PersistentClient(path=str(DB_PATH))
        
        # í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ë§¤ë²ˆ ì´ˆê¸°í™” (í•„ìš” ì‹œ ì´ ë¶€ë¶„ ì£¼ì„ ì²˜ë¦¬)
        try:
            self.client.delete_collection(COLLECTION_NAME)
            logger.info(f"ğŸ—‘ï¸ ê¸°ì¡´ ì»¬ë ‰ì…˜ '{COLLECTION_NAME}' ì´ˆê¸°í™” ì™„ë£Œ")
        except: pass
        
        self.collection = self.client.get_or_create_collection(
            name=COLLECTION_NAME, 
            metadata={"hnsw:space": "cosine"}
        )
        
        # ëª¨ë¸ ë¡œë“œ
        self.model = SentenceTransformer(EMBED_MODEL_NAME)
        logger.info(f"âœ… ëª¨ë¸ ë° DB ì¤€ë¹„ ì™„ë£Œ ({EMBED_MODEL_NAME})")

    def extract_text(self, file_path: Path) -> str:
        """íŒŒì¼ í˜•ì‹ë³„ í…ìŠ¤íŠ¸ ì¶”ì¶œ (í•˜ìœ„ ë””ë ‰í† ë¦¬ ëŒ€ì‘)"""
        ext = file_path.suffix.lower()
        text = ""
        try:
            if ext == '.pdf':
                reader = PdfReader(file_path)
                text = " ".join([page.extract_text() for page in reader.pages if page.extract_text()])
            elif ext == '.docx':
                doc = Document(file_path)
                text = " ".join([p.text for p in doc.paragraphs])
            elif ext == '.pptx':
                prs = Presentation(file_path)
                for slide in prs.slides:
                    for shape in slide.shapes:
                        if hasattr(shape, "text"): text += shape.text + " "
            elif ext == '.txt':
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    text = f.read()
        except Exception as e:
            logger.error(f"âŒ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨ ({file_path.name}): {e}")
        return text

    def get_chunks(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ë¥¼ ê³ ì • í¬ê¸° ì²­í¬ë¡œ ë¶„í• """
        if not text: return []
        chunks = []
        for i in range(0, len(text), CHUNK_SIZE - CHUNK_OVERLAP):
            chunks.append(text[i : i + CHUNK_SIZE])
        return chunks

    def run(self):
        start_time = datetime.now()
        # 1. íŒŒì¼ ìŠ¤ìº” (í•˜ìœ„ ë””ë ‰í† ë¦¬ í¬í•¨)
        all_files = []
        for root, _, filenames in os.walk(TARGET_DIR):
            for f in filenames:
                p = Path(root) / f
                if p.suffix.lower() in {'.pdf', '.docx', '.pptx', '.txt'}:
                    all_files.append(p)

        logger.info(f"ğŸ” ì´ {len(all_files)}ê°œì˜ íŒŒì¼ ìŠ¤ìº” ì™„ë£Œ. ì²˜ë¦¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")

        total_chunks_count = 0
        
        # 2. íŒŒì¼ë³„ ë£¨í”„
        for idx, file_path in enumerate(all_files, 1):
            raw_text = self.extract_text(file_path)
            chunks = self.get_chunks(raw_text)
            
            if not chunks:
                logger.warning(f"âš ï¸ [{idx}/{len(all_files)}] {file_path.name} - ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ì—†ìŒ")
                continue

            # 3. ë°°ì¹˜ ì„ë² ë”© ë° ì €ì¥ (íŒŒì¼ ë‹¨ìœ„)
            try:
                # íŒŒì¼ ë‚´ ëª¨ë“  ì²­í¬ë¥¼ í•œ ë²ˆì— ì„ë² ë”© (ì†ë„ ìµœì í™”)
                embeddings = self.model.encode(chunks).tolist()
                
                # ê³ ìœ  ID ìƒì„± (íŒŒì¼ëª… + ì²­í¬ ìˆœë²ˆì˜ í•´ì‹œ)
                ids = [hashlib.md5(f"{file_path.name}_{i}".encode()).hexdigest() for i in range(len(chunks))]
                
                metadatas = [{
                    "source": str(file_path),
                    "filename": file_path.name,
                    "date": datetime.now().isoformat()
                } for _ in chunks]

                self.collection.add(
                    ids=ids,
                    embeddings=embeddings,
                    documents=chunks,
                    metadatas=metadatas
                )
                
                total_chunks_count += len(chunks)
                logger.info(f"âœ… [{idx}/{len(all_files)}] {file_path.name} - {len(chunks)}ê°œ ì²­í¬ ì €ì¥")

            except Exception as e:
                logger.error(f"âŒ [{idx}/{len(all_files)}] ì €ì¥ ì‹¤íŒ¨: {e}")

        duration = datetime.now() - start_time
        logger.info(f"ğŸ ì „ì²´ ì²˜ë¦¬ ì™„ë£Œ! ì´ {total_chunks_count}ê°œ ì²­í¬ ì €ì¥ (ì†Œìš”ì‹œê°„: {duration})")

if __name__ == "__main__":
    loader = RobustRAGLoader()
    loader.run()
