#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import logging
import hashlib
import fitz  # PyMuPDF
from pathlib import Path
from datetime import datetime
from typing import List, Dict

# ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ
import chromadb
from openai import OpenAI
from PyPDF2 import PdfReader
from docx import Document
from pptx import Presentation

# 1. í™˜ê²½ ì„¤ì •
TARGET_DIR = Path(r"C:/Users/USER/Downloads/@@@ì¸ë„ë„¤ì‹œì•„PDTì•”ì„¼í„°FS")
DB_PATH = Path(r"C:/Users/USER/rag/src/data/chroma_db")
COLLECTION_NAME = "indonesia_pdt_docs"
# OpenAI ì„¤ì •
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY") # ì—¬ê¸°ì— í‚¤ë¥¼ ë„£ê±°ë‚˜ í™˜ê²½ë³€ìˆ˜ ì‚¬ìš©
EMBED_MODEL = "text-embedding-3-small" # ê°€ì„±ë¹„ì™€ ì„±ëŠ¥ì´ ê°€ì¥ ì¢‹ì€ ìµœì‹  ëª¨ë¸

CHUNK_SIZE = 800
CHUNK_OVERLAP = 100

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class RobustRAGLoaderV2:
    def __init__(self):
        logger.info("ğŸš€ ì‹œìŠ¤í…œ ì´ˆê¸°í™” (OpenAI Embedding Mode)")
        self.client = OpenAI(api_key=OPENAI_API_KEY)
        
        # ChromaDB ì´ˆê¸°í™”
        self.db_client = chromadb.PersistentClient(path=str(DB_PATH))
        
        # í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ì´ˆê¸°í™”
        try:
            self.db_client.delete_collection(COLLECTION_NAME)
            logger.info(f"ğŸ—‘ï¸ ê¸°ì¡´ ì»¬ë ‰ì…˜ '{COLLECTION_NAME}' ì´ˆê¸°í™” ì™„ë£Œ")
        except: pass
        
        self.collection = self.db_client.create_collection(
            name=COLLECTION_NAME,
            metadata={"hnsw:space": "cosine"}
        )

    # ... ê¸°ì¡´ import ìœ ì§€
    def extract_text(self, file_path: Path) -> str:
        """íŒŒì¼ë³„ í…ìŠ¤íŠ¸ ì¶”ì¶œ (PyMuPDF ì—”ì§„ + ìš©ì–´ ë³´ì •)"""
        ext = file_path.suffix.lower()
        text = ""
        try:
            # 1. íŒŒì¼ í™•ì¥ìë³„ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            if ext == '.pdf':
                # PyMuPDF(fitz) ì‚¬ìš© - í•œê¸€ ì¶”ì¶œ ëŠ¥ë ¥ ìš°ìˆ˜
                doc = fitz.open(file_path)
                text = " ".join([page.get_text() for page in doc])
                doc.close()
            elif ext == '.docx':
                from docx import Document
                doc = Document(file_path)
                text = "\n".join([p.text for p in doc.paragraphs])
            elif ext == '.pptx':
                from pptx import Presentation
                prs = Presentation(file_path)
                text = " ".join([shape.text for slide in prs.slides for shape in slide.shapes if hasattr(shape, "text")])
            elif ext == '.txt':
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    text = f.read()

            # 2. í•œê¸€ ì¸ì½”ë”© ì˜¤ë¥˜ ìš©ì–´ ë³´ì • (Glossary Correction)
            if text:
                corrections = {
                    "ì‹": "ì‹ ", "ì§‚": "ì§„", "ì¶": "ì¸", "í•š": "í•œ", 
                    "ì›": "ì›", "ìƒí™–": "ìƒí™˜", "ì·": "ì¼", "ì ‚": "ì „",
                    "ê³¾": "ê´€", "ì²š": "ì²œ", "ì¤‘ê°‚": "ì¤‘ê°„", "ì–¶": "ì–¸",
                    "ì»": "ì¸", "ì‹µ": "ì‹¤", "ì—“": "ì—‘", "ì§—": "ì§€"
                }
                for wrong, right in corrections.items():
                    text = text.replace(wrong, right)

        except Exception as e:
            logger.error(f"âŒ {file_path.name} ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        
        return text


    def run(self):
        all_files = []
        for root, _, filenames in os.walk(TARGET_DIR):
            for f in filenames:
                p = Path(root) / f
                if p.suffix.lower() in {'.pdf', '.docx', '.pptx', '.txt'}:
                    all_files.append(p)

        logger.info(f"ğŸ” ì´ {len(all_files)}ê°œ íŒŒì¼ ì²˜ë¦¬ ì‹œì‘")

        total_chunks = 0
        for idx, file_path in enumerate(all_files, 1):
            raw_text = self.extract_text(file_path)
            if not raw_text: continue

            # ë‹¨ìˆœ ì²­í‚¹
            chunks = [raw_text[i:i+CHUNK_SIZE] for i in range(0, len(raw_text), CHUNK_SIZE - CHUNK_OVERLAP)]
            
            try:
                # OpenAI ë°°ì¹˜ ì„ë² ë”© (ì—¬ëŸ¬ ì²­í¬ë¥¼ í•œ ë²ˆì˜ í˜¸ì¶œë¡œ ì²˜ë¦¬)
                response = self.client.embeddings.create(input=chunks, model=EMBED_MODEL)
                embeddings = [data.embedding for data in response.data]
                # ê°€ì§œ ì„ë² ë”© ìƒì„± (ëª¨ë¸ í¬ê¸° 1536ì— ë§ì¶˜ ëœë¤ê°’)
                # import numpy as np
                # embeddings = np.random.rand(len(chunks), 1536).tolist()
    
                # ì´ì œ ì•„ë˜ ì €ì¥ ë¡œì§ì´ ì •ìƒ ì‘ë™í•˜ë©° DBì— ìˆ«ìê°€ ìŒ“ì¼ ê²ë‹ˆë‹¤!
                ids = [hashlib.md5(f"{file_path.name}_{i}".encode()).hexdigest() for i in range(len(chunks))]
                self.collection.add(
                    ids=ids,
                    embeddings=embeddings,
                    documents=chunks,
                    metadatas=[{"source": file_path.name} for _ in chunks]
                )
                total_chunks += len(chunks)

                logger.info(f"âœ… [{idx}/{len(all_files)}] {file_path.name} ({len(chunks)} Chunks)")
            
            except Exception as e:
                logger.error(f"âŒ [{idx}/{len(all_files)}] {file_path.name} ì²˜ë¦¬ ì¤‘ ì—ëŸ¬: {e}")

        logger.info(f"ğŸ ì™„ë£Œ! ì´ {total_chunks}ê°œ ì²­í¬ ì €ì¥ë¨.")

if __name__ == "__main__":
    loader = RobustRAGLoaderV2()
    loader.run()
