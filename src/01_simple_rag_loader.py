#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import logging
import hashlib
import re
import fitz  # PyMuPDF
from pathlib import Path
from datetime import datetime
from typing import List, Dict

# ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ
import chromadb
from openai import OpenAI
from docx import Document
from pptx import Presentation

# 1. í™˜ê²½ ì„¤ì •
TARGET_DIR = Path(r"C:/Users/USER/Downloads/@@@ì¸ë„ë„¤ì‹œì•„PDTì•”ì„¼í„°FS")
DB_PATH = Path(r"C:/Users/USER/rag/src/data/chroma_db")
COLLECTION_NAME = "indonesia_pdt_docs"

# OpenAI ì„¤ì •
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY") 
EMBED_MODEL = "text-embedding-3-small"

CHUNK_SIZE = 800
CHUNK_OVERLAP = 100

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class RobustRAGLoaderV2:
    def __init__(self):
        logger.info("ğŸš€ ì‹œìŠ¤í…œ ì´ˆê¸°í™” (OpenAI Embedding Mode)")
        self.client = OpenAI(api_key=OPENAI_API_KEY)
        
        # ChromaDB ì´ˆê¸°í™”
        self.db_client = chromadb.PersistentClient(path=str(DB_PATH))
        
        # [ì£¼ì˜] ëŒ€ìš©ëŸ‰ ì²˜ë¦¬ ì‹œì—ëŠ” delete_collectionì„ ì£¼ì„ ì²˜ë¦¬í•´ì•¼ 'ì´ì–´ì„œ í•˜ê¸°'ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤.
        # ì²˜ìŒë¶€í„° ì™„ì „íˆ ìƒˆë¡œ í•˜ê³  ì‹¶ì„ ë•Œë§Œ ì•„ë˜ ë‘ ì¤„ì˜ ì£¼ì„ì„ í•´ì œí•˜ì„¸ìš”.
        # try:
        #     self.db_client.delete_collection(COLLECTION_NAME)
        #     logger.info(f"ğŸ—‘ï¸ ê¸°ì¡´ ì»¬ë ‰ì…˜ '{COLLECTION_NAME}' ì´ˆê¸°í™” ì™„ë£Œ")
        # except: pass
        
        self.collection = self.db_client.get_or_create_collection(
            name=COLLECTION_NAME,
            metadata={"hnsw:space": "cosine"}
        )

    def extract_text(self, file_path: Path) -> str:
        """íŒŒì¼ë³„ í…ìŠ¤íŠ¸ ì¶”ì¶œ (PyMuPDF ì—”ì§„ + ìš©ì–´ ë³´ì • + ì¤‘ë³µ ì œê±°)"""
        ext = file_path.suffix.lower()
        text = ""
        try:
            # 1. íŒŒì¼ í™•ì¥ìë³„ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            if ext == '.pdf':
                doc = fitz.open(file_path)
                text = " ".join([page.get_text() for page in doc])
                doc.close()
            elif ext == '.docx':
                doc = Document(file_path)
                text = "\n".join([p.text for p in doc.paragraphs])
            elif ext == '.pptx':
                prs = Presentation(file_path)
                text = " ".join([shape.text for slide in prs.slides for shape in slide.shapes if hasattr(shape, "text")])
            elif ext == '.txt':
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    text = f.read()

            # 2. í…ìŠ¤íŠ¸ ë³´ì • ë¡œì§
            if text:
                # (1) í•œê¸€ ì¸ì½”ë”© ì˜¤ë¥˜ êµì •
                corrections = {
                    "ì‹": "ì‹ ", "ì§‚": "ì§„", "ì¶": "ì¸", "í•š": "í•œ", 
                    "ì›": "ì›", "ìƒí™–": "ìƒí™˜", "ì·": "ì¼", "ì ‚": "ì „",
                    "ê³¾": "ê´€", "ì²š": "ì²œ", "ì¤‘ê°‚": "ì¤‘ê°„", "ì–¶": "ì–¸",
                    "ì»": "ì¸", "ì‹µ": "ì‹¤", "ì—“": "ì—‘", "ì§—": "ì§€",
                    "ìƒ": "ì€", "ì”": "ì–µ", "ì‹²": "ì‹œ", "ì§˜": "ì§"
                }
                for wrong, right in corrections.items():
                    text = text.replace(wrong, right)
                
                # (2) ì¤‘ë³µ ë‹¨ì–´ ì œê±° (ì˜ˆ: ì‹ ì¶•ì‚¬ì—…ì¶•ì‚¬ì—… -> ì‹ ì¶•ì‚¬ì—…)
                text = re.sub(r'([ê°€-í£]{2,})\1', r'\1', text)

        except Exception as e:
            logger.error(f"âŒ {file_path.name} ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        
        return text

    def run(self):
        # 1. ëª¨ë“  íŒŒì¼ ìŠ¤ìº” (í•˜ìœ„ ë””ë ‰í† ë¦¬ í¬í•¨)
        all_files = []
        for root, _, filenames in os.walk(TARGET_DIR):
            for f in filenames:
                p = Path(root) / f
                if p.suffix.lower() in {'.pdf', '.docx', '.pptx', '.txt'}:
                    all_files.append(p)

        logger.info(f"ğŸ” ì´ {len(all_files)}ê°œ íŒŒì¼ ë°œê²¬")

        # 2. ì´ë¯¸ DBì— ì €ì¥ëœ íŒŒì¼ í™•ì¸ (ì¤‘ë³µ ë°©ì§€ ë° ì´ì–´ì„œ í•˜ê¸°)
        existing_data = self.collection.get(include=['metadatas'])
        processed_files = {m['source'] for m in existing_data['metadatas']} if existing_data['metadatas'] else set()

        total_chunks = 0
        for idx, file_path in enumerate(all_files, 1):
            # ì´ë¯¸ ì²˜ë¦¬í•œ íŒŒì¼ì€ ê±´ë„ˆëœ€ (íŒŒì¼ëª…+ì–¸ë”ë°” ê·œì¹™ ì¤€ìˆ˜ ì‹œ ìë™ í•„í„°ë§)
            if file_path.name in processed_files:
                continue

            raw_text = self.extract_text(file_path)
            if not raw_text.strip():
                continue

            # ì²­í‚¹ ì‘ì—…
            chunks = [raw_text[i:i+CHUNK_SIZE] for i in range(0, len(raw_text), CHUNK_SIZE - CHUNK_OVERLAP)]
            
            try:
                # OpenAI ì„ë² ë”© ìƒì„±
                response = self.client.embeddings.create(input=chunks, model=EMBED_MODEL)
                embeddings = [data.embedding for data in response.data]
                
                # ID ìƒì„± ë° ì €ì¥
                ids = [hashlib.md5(f"{file_path.name}_{i}".encode()).hexdigest() for i in range(len(chunks))]
                self.collection.add(
                    ids=ids,
                    embeddings=embeddings,
                    documents=chunks,
                    metadatas=[{"source": file_path.name} for _ in chunks]
                )
                total_chunks += len(chunks)
                logger.info(f"âœ… [{idx}/{len(all_files)}] {file_path.name} ({len(chunks)} Chunks)")
            
            except Exception as e:
                logger.error(f"âŒ [{idx}/{len(all_files)}] {file_path.name} ì²˜ë¦¬ ì¤‘ ì—ëŸ¬: {e}")

        logger.info(f"ğŸ ì™„ë£Œ! í˜„ì¬ ì»¬ë ‰ì…˜ ë‚´ ì´ ì²­í¬ ìˆ˜: {self.collection.count()}")

if __name__ == "__main__":
    loader = RobustRAGLoaderV2()
    loader.run()
