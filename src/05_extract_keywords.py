import os
from collections import Counter
import pandas as pd
from kiwipiepy import Kiwi
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings
from dotenv import load_dotenv

# .env ë¡œë“œ
load_dotenv()

# ê²½ë¡œ ì„¤ì • (ì‚¬ìš©ìë‹˜ í™˜ê²½ ìœ ì§€)
DB_PATH = r"C:/Users/USER/rag/src/data/chroma_db"
COLLECTION_NAME = "indonesia_pdt_docs"
OUTPUT_FILE = "top_300_keywords.csv"

def extract_keywords():
    print("ğŸš€ í‚¤ì›Œë“œ ë¶„ì„ ì—”ì§„ ê°€ë™ ì¤‘...")
    
    # 1. í˜•íƒœì†Œ ë¶„ì„ê¸°(Kiwi) ì´ˆê¸°í™”
    kiwi = Kiwi()
    
    # 2. DB ì—°ê²° (ê¸°ì¡´ ë°ì´í„° ë¡œë“œ)
    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
    vector_db = Chroma(
        persist_directory=DB_PATH,
        embedding_function=embeddings,
        collection_name=COLLECTION_NAME
    )
    
    # 3. ë°ì´í„° ì´ ê°œìˆ˜ í™•ì¸
    total_count = vector_db._collection.count()
    print(f"ğŸ“Š ì´ ë°ì´í„° ìœ ë‹›(Chunks): {total_count}ê°œ")
    
    # 4. ë°°ì¹˜ ë‹¨ìœ„ë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ë‹¨ì–´ ì¹´ìš´íŠ¸
    word_counter = Counter()
    batch_size = 500  # ë©”ëª¨ë¦¬ ë³´í˜¸ë¥¼ ìœ„í•´ 500ê°œì”© ì²˜ë¦¬
    
    print(f"ğŸ” ë‹¨ì–´ ë¹ˆë„ ë¶„ì„ ì‹œì‘ (Batch Size: {batch_size})...")
    
    for i in range(0, total_count, batch_size):
        # DBì—ì„œ í…ìŠ¤íŠ¸ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (offset í™œìš©)
        results = vector_db._collection.get(
            limit=batch_size,
            offset=i,
            include=["documents"]
        )
        
        batch_texts = results.get("documents", [])
        
        for text in batch_texts:
            # ëª…ì‚¬(Noun)ë§Œ ì¶”ì¶œ (ê¸¸ì´ 2ì ì´ìƒ)
            tokens = kiwi.tokenize(text)
            nouns = [t.form for t in tokens if t.tag.startswith('N') and len(t.form) > 1]
            word_counter.update(nouns)
            
        if (i // batch_size) % 10 == 0 or (i + batch_size) >= total_count:
            current_progress = min(i + batch_size, total_count)
            print(f"â³ ì§„í–‰ë¥ : {current_progress} / {total_count} ({current_progress/total_count*100:.1f}%)")

    # 5. ìƒìœ„ 300ê°œ ì¶”ì¶œ
    top_300 = word_counter.most_common(300)
    
    # 6. ê²°ê³¼ ì €ì¥ ë° ì¶œë ¥
    df = pd.DataFrame(top_300, columns=['Keyword', 'Frequency'])
    df.to_csv(OUTPUT_FILE, index=False, encoding='utf-8-sig')
    
    print("\n" + "="*30)
    print(f"ğŸ ë¶„ì„ ì™„ë£Œ! ìƒìœ„ 10ê°œ í‚¤ì›Œë“œ:")
    print(df.head(10))
    print(f"\nğŸ“‚ ì „ì²´ ë¦¬ìŠ¤íŠ¸ê°€ '{OUTPUT_FILE}'ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print("="*30)

if __name__ == "__main__":
    extract_keywords()
