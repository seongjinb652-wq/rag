import os
import logging
from pathlib import Path
from langchain_openai import OpenAIEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from dotenv import load_dotenv

# .env ë¡œë“œ
load_dotenv()

# ê²½ë¡œ ë° ì„¤ì •
TXT_DIR = Path(r"C:/Users/USER/rag/src/data/text_converted")
DB_PATH = r"C:/Users/USER/rag/src/data/chroma_db"
COLLECTION_NAME = "indonesia_pdt_docs"

def load_incremental():
    # 1. ì´ˆê¸°í™”(shutil.rmtree) ë¡œì§ ì‚­ì œ -> ê¸°ì¡´ DB ìœ ì§€
    if not os.path.exists(DB_PATH):
        print(f"ğŸ“‚ DBê°€ ì¡´ì¬í•˜ì§€ ì•Šì•„ ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤: {DB_PATH}")
    else:
        print(f"ğŸ“š ê¸°ì¡´ DBì— ë°ì´í„°ë¥¼ ì´ì–´ ì”ë‹ˆë‹¤: {DB_PATH}")

    # 2. ëª¨ë¸ ë° ìŠ¤í”Œë¦¬í„° ì„¤ì •
    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000, 
        chunk_overlap=150,
        separators=["\n\n", "\n", " ", ""]
    )

    # 3. íŒŒì¼ ëª©ë¡ (ì¤‘ë³µ ì ì¬ ë°©ì§€ë¥¼ ìœ„í•´ ê³ ë¯¼ì´ í•„ìš”í•˜ì§€ë§Œ, ì¼ë‹¨ ì „ì²´ ë¡œë“œ)
    all_files = list(TXT_DIR.glob("*.txt"))
    print(f"ğŸš€ ì´ {len(all_files)}ê°œ íŒŒì¼ ì²˜ë¦¬ ì‹œì‘ (ê¸°ì¡´ ë°ì´í„° ìœ ì§€)...")

    # 4. Chroma DB ì—°ê²° (ê¸°ì¡´ ê²½ë¡œ ë¡œë“œ)
    vector_db = Chroma(
        persist_directory=DB_PATH,
        embedding_function=embeddings,
        collection_name=COLLECTION_NAME
    )

    # 5. ë°°ì¹˜ ì²˜ë¦¬
    batch_size = 20 
    for i in range(0, len(all_files), batch_size):
        batch_files = all_files[i : i + batch_size]
        texts = []
        metadatas = []

        for file_path in batch_files:
            try:
                with open(file_path, "r", encoding="utf-8") as f:
                    content = f.read()
                    chunks = text_splitter.split_text(content)
                    for chunk in chunks:
                        texts.append(chunk)
                        metadatas.append({"source": file_path.name})
            except Exception as e:
                print(f"âŒ ì˜¤ë¥˜ ({file_path.name}): {e}")

        if texts:
            vector_db.add_texts(texts=texts, metadatas=metadatas)
            print(f"âœ… ë°°ì¹˜ ì™„ë£Œ: {min(i + batch_size, len(all_files))} / {len(all_files)}")

    print(f"ğŸ ì¦ë¶„ ì ì¬ ì™„ë£Œ! ìœ„ì¹˜: {DB_PATH}")

if __name__ == "__main__":
    load_incremental()
