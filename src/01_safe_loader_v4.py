import os
import logging
from pathlib import Path
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_text_splitters import RecursiveCharacterTextSplitter
from dotenv import load_dotenv
import time

# .env íŒŒì¼ ë¡œë“œ
load_dotenv() 

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

# í™•ì¸ìš©
if OPENAI_API_KEY:
    print(f"ğŸ”‘ API KEY ë¡œë“œ ì„±ê³µ: {OPENAI_API_KEY[:5]}*****")
else:
    print("âŒ .env íŒŒì¼ì—ì„œ OPENAI_API_KEYë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

# [ì„¤ì •] ê²½ë¡œëŠ” v3ì™€ ë™ì¼í•˜ê²Œ ìœ ì§€ (ì‚¬ìš©ìë‹˜ì´ ìœˆë„ìš°ì—ì„œ íŒŒì¼ë§Œ êµì²´)
TXT_DIR = Path(r"C:/Users/USER/rag/src/data/text_converted")
DB_PATH = r"C:/Users/USER/rag/src/data/chroma_db"
COLLECTION_NAME = "indonesia_pdt_docs"

def append_to_existing_db():
    # 1. ëª¨ë¸ ë° ìŠ¤í”Œë¦¬í„° ì„¤ì • (v3ì™€ ë™ì¼í•˜ê²Œ ìœ ì§€í•´ì•¼ ë°ì´í„° ì¼ê´€ì„± ë³´ì¥)
    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000, 
        chunk_overlap=150,
        separators=["\n\n", "\n", " ", ""]
    )

    # 2. [v4 í•µì‹¬] ê¸°ì¡´ DB ë¶ˆëŸ¬ì˜¤ê¸° (ì‚­ì œ ë¡œì§ ì—†ìŒ)
    if os.path.exists(DB_PATH):
        print(f"ğŸ“¦ ê¸°ì¡´ DB ë¡œë“œ ì¤‘: {DB_PATH}")
        vector_db = Chroma(
            persist_directory=DB_PATH,
            embedding_function=embeddings,
            collection_name=COLLECTION_NAME
        )
    else:
        print("âŒ ê¸°ì¡´ DBë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        return

    # 3. ìƒˆë¡œìš´ íŒŒì¼ ëª©ë¡ (3.2GB íŒŒì¼ë“¤ì´ ìˆëŠ” í´ë”)
    all_files = list(TXT_DIR.glob("*.txt"))
    print(f"ğŸš€ ì´ {len(all_files)}ê°œ íŒŒì¼ ì¶”ê°€ ì ì¬ ì‹œì‘...")

    # 4. ë°°ì¹˜ ì²˜ë¦¬ (ì‚¬ìš©ìë‹˜ ìµœì í™” ì„¤ì • ì ìš©)
    batch_size = 15  # íŒŒì¼ 15ê°œì”© ì½ê¸°
    for i in range(0, len(all_files), batch_size):
        batch_files = all_files[i : i + batch_size]
        texts = []
        metadatas = []

        for file_path in batch_files:
            try:
                with open(file_path, "r", encoding="utf-8") as f:
                    content = f.read()
                    chunks = text_splitter.split_text(content)
                    for chunk in chunks:
                        texts.append(chunk)
                        metadatas.append({"source": file_path.name})
            except Exception as e:
                print(f"âŒ ì˜¤ë¥˜ ({file_path.name}): {e}")

        # 5. DBì— ë°ì´í„° ì¶”ê°€ (ì•ˆì „í•œ ì¬ì‹œë„ ë¡œì§ í¬í•¨)
        if texts:
            text_batch_limit = 100 
            for j in range(0, len(texts), text_batch_limit):
                sub_texts = texts[j : j + text_batch_limit]
                sub_metadatas = metadatas[j : j + text_batch_limit]
                
                success = False
                while not success:
                    try:
                        # add_textsë¥¼ í†µí•´ ê¸°ì¡´ ì»¬ë ‰ì…˜ì— ì¶”ê°€
                        vector_db.add_texts(texts=sub_texts, metadatas=sub_metadatas)
                        
                        # ì‚¬ìš©ìë‹˜ ì„¤ì •ê°’: 0.2ì´ˆ íœ´ì‹
                        time.sleep(0.2) 
                        success = True
                    except Exception as e:
                        if "429" in str(e):
                            print("â³ ì†ë„ ì œí•œ(429) ê°ì§€. 10ì´ˆ ëŒ€ê¸° í›„ ë‹¤ì‹œ ì‹œë„í•©ë‹ˆë‹¤...")
                            time.sleep(10)
                        else:
                            print(f"âŒ ë°ì´í„° ì¶”ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                            break # ì¹˜ëª…ì  ì—ëŸ¬ ì‹œ ì¤‘ë‹¨
            
            print(f"âœ… ì¶”ê°€ ì™„ë£Œ: {min(i + batch_size, len(all_files))} / {len(all_files)}")

    print(f"ğŸ ëª¨ë“  ë°ì´í„° ì¶”ê°€ ì™„ë£Œ! ìœ„ì¹˜: {DB_PATH}")

if __name__ == "__main__":
    append_to_existing_db()
