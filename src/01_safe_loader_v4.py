import os
import json
import shutil
import time
import logging
from datetime import datetime
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_text_splitters import RecursiveCharacterTextSplitter
from config import Settings  # ì¤‘ì•™ ì„¤ì • ì°¸ì¡° (RESET_DB í¬í•¨)

# 1. ì—ëŸ¬ ë¡œê·¸ ì„¤ì • (logs í´ë”ì— ì¼ìë³„ ê¸°ë¡)
log_file_path = Settings.LOGS_DIR / f"loader_error_{datetime.now().strftime('%Y%m%d')}.log"
logging.basicConfig(
    filename=log_file_path,
    level=logging.ERROR,
    format='%(asctime)s - %(levelname)s - %(message)s',
    encoding='utf-8'
)

def get_db_status(vector_db):
    """í˜„ì¬ DBì˜ ì»¬ë ‰ì…˜ ë‚´ ì²­í¬ ê°œìˆ˜ í™•ì¸"""
    try:
        return vector_db._collection.count()
    except Exception:
        return 0

def process_and_save():
    """
    [í†µí•© ì„¸ì´í”„ ë¡œë” v4]
    - v3 ê¸°ëŠ¥: Settings.RESET_DB = True ì‹œ ê¸°ì¡´ DB ì‚­ì œ í›„ ì¬ì‹œì‘
    - v4 ê¸°ëŠ¥: Settings.RESET_DB = False ì‹œ ì¤‘ë‹¨ëœ ì§€ì ë¶€í„° ì´ì–´ë„£ê¸°
    """
    # ê¸°ë³¸ ê²½ë¡œ ì„¤ì •
    db_path = str(Settings.CHROMA_DB_PATH)
    state_file = Settings.BATCH_STATE_FILE
    input_dir = Settings.DATA_DIR / "text_converted"
    embeddings = OpenAIEmbeddings(model=Settings.EMBEDDING_MODEL)
    
    # [v3 ì´ˆê¸°í™” ì ˆì°¨]
    if Settings.RESET_DB:
        now = datetime.now().strftime("%H:%M:%S")
        print(f"[{now}] âš ï¸ [v3 ëª¨ë“œ] ì´ˆê¸°í™”ë¥¼ ìœ„í•´ ê¸°ì¡´ ë°ì´í„°ë¥¼ ì‚­ì œí•©ë‹ˆë‹¤.")
        if os.path.exists(db_path):
            shutil.rmtree(db_path)
            print(f"[{now}] ğŸ—‘ï¸  DB í´ë” ì‚­ì œ ì™„ë£Œ.")
        if os.path.exists(state_file):
            os.remove(state_file)
            print(f"[{now}] ğŸ—‘ï¸  ìƒíƒœ ê¸°ë¡ íŒŒì¼ ì‚­ì œ ì™„ë£Œ.")

    # 2. ë²¡í„° DB ì—°ê²°
    vector_db = Chroma(
        persist_directory=db_path,
        embedding_function=embeddings,
        collection_name=Settings.CHROMA_COLLECTION_NAME
    )

    # 3. ìƒíƒœ í™•ì¸
    initial_count = get_db_status(vector_db)
    processed_files = set()
    if not Settings.RESET_DB and os.path.exists(state_file):
        with open(state_file, "r", encoding="utf-8") as f:
            processed_files = set(json.load(f))

    # 4. ëŒ€ìƒ íŒŒì¼ ëª©ë¡ ì¶”ì¶œ
    all_files = [f for f in os.listdir(input_dir) if f.endswith(".txt")]
    files_to_process = [f for f in all_files if f not in processed_files]
    total_files = len(files_to_process)
    
    # 5. í†µê³„ ë° ë°°ì¹˜ ê´€ë¦¬ ë³€ìˆ˜
    batch_size = 20         # 20ê°œ íŒŒì¼ë§ˆë‹¤ ë°°ì¹˜ ë³´ê³ 
    current_batch_chunks = 0
    total_added_chunks = 0
    
    print(f"\nğŸ“Š [DB í˜„í™©] ê¸°ì¡´ ë°ì´í„°: {initial_count}ê±´")
    print(f"ğŸš€ [ì‘ì—… ì‹œì‘] ì²˜ë¦¬ ëŒ€ìƒ: {total_files}ê°œ íŒŒì¼ (ë¡œê·¸: {log_file_path.name})\n")

    # ìŠ¤í”Œë¦¬í„° ì„¤ì • (ì˜ë¯¸ ë‹¨ìœ„ ë¶„ë¦¬ë¥¼ ìœ„í•œ separators í¬í•¨)
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=Settings.CHUNK_SIZE,
        chunk_overlap=Settings.CHUNK_OVERLAP,
        separators=["\n\n", "\n", " ", ""]
    )

    # 6. ë©”ì¸ ì²˜ë¦¬ ë£¨í”„
    for idx, file_name in enumerate(files_to_process, 1):
        file_path = os.path.join(input_dir, file_name)
        now_time = datetime.now().strftime("%H:%M:%S")
        
        try:
            # íŒŒì¼ ì½ê¸° (ì¸ì½”ë”© ì—ëŸ¬ ë¬´ì‹œ ì„¤ì •ìœ¼ë¡œ ì•ˆì •ì„± í™•ë³´)
            with open(file_path, "r", encoding="utf-8", errors="ignore") as f:
                full_content = f.read()
            
            lines = full_content.split('\n')
            
            # [ì¶œì²˜ ë³µì›] 00ë²ˆ ì»¨ë²„í„°ì—ì„œ ë„£ì€ "Source:" í—¤ë” íŒŒì‹±
            if lines and lines[0].startswith("Source:"):
                full_source_path = lines[0].replace("Source:", "").strip()
                original_name = full_source_path.replace('\\', '/').split('/')[-1]
                # ë³¸ë¬¸ ì •ì œ: í—¤ë”(0)ì™€ êµ¬ë¶„ì„ (1) ì œì™¸í•˜ê³  2ë²ˆ ì¤„ë¶€í„°
                content_body = "\n".join(lines[2:]).strip()
            else:
                original_name = file_name.rsplit('_', 1)[0]
                content_body = full_content

            # ì²­í¬ ìƒì„±
            chunks = text_splitter.split_text(content_body)
            num_chunks = len(chunks)
            metadatas = [{Settings.META_SOURCE_KEY: original_name} for _ in range(num_chunks)]

            # [ì•ˆì •ì„± ì ì¬ ë£¨í”„] Rate Limit ëŒ€ì‘
            success = False
            while not success:
                try:
                    vector_db.add_texts(texts=chunks, metadatas=metadatas)
                    time.sleep(0.2)  # ì¸ë² ë”© ê³¼ë¶€í•˜ ë°©ì§€ íœ´ì‹
                    success = True
                except Exception as e:
                    if "429" in str(e):
                        print(f"\n[{now_time}] â³ [Rate Limit] 10ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„... ({idx}/{total_files})")
                        time.sleep(10)
                    else:
                        raise e # ë‹¤ë¥¸ ì—ëŸ¬ëŠ” ìƒìœ„ exceptë¡œ ì „ë‹¬

            # í†µê³„ ì—…ë°ì´íŠ¸ ë° ì§„í–‰ ìƒíƒœ ì €ì¥
            current_batch_chunks += num_chunks
            total_added_chunks += num_chunks
            processed_files.add(file_name)
            
            with open(state_file, "w", encoding="utf-8") as f:
                json.dump(list(processed_files), f, ensure_ascii=False, indent=4)
            
            # [í™”ë©´ ì¶œë ¥ ë¡œì§]
            # í° íŒŒì¼(ì²­í¬ 20ê°œ ì´ìƒ) ì•Œë¦¼
            if num_chunks >= 20:
                print(f"\n[{now_time}] ğŸ˜ [ëŒ€í˜•] ({idx}/{total_files}) {original_name} (ì²­í¬: {num_chunks}ê°œ)")
            
            # ë°°ì¹˜ ë‹¨ìœ„ ë³´ê³ 
            if idx % batch_size == 0:
                print(f"\n[{now_time}] ğŸ“¦ [ë°°ì¹˜ì™„ë£Œ] {idx}ë²ˆê¹Œì§€ ì²˜ë¦¬ ì™„ë£Œ (í˜„ì¬ ë°°ì¹˜ ì²­í¬: {current_batch_chunks}ê°œ)")
                current_batch_chunks = 0
            else:
                # ì¼ë°˜ íŒŒì¼ì€ í•œ ì¤„ ê°±ì‹ 
                print(f"\r[{now_time}] ({idx}/{total_files}) ì²˜ë¦¬ ì¤‘: {original_name[:25]}...", end="")

        except Exception as e:
            err_msg = f"ì‹¤íŒ¨: {file_name} | ì´ìœ : {str(e)}"
            print(f"\n[{now_time}] âŒ {err_msg}")
            logging.error(err_msg)

    # 7. ìµœì¢… ê²°ê³¼ ë¦¬í¬íŠ¸
    final_count = get_db_status(vector_db)
    print("\n\n" + "="*60)
    print("ğŸ ëª¨ë“  ë°ì´í„° ì ì¬ ì™„ë£Œ")
    print("-" * 60)
    print(f"ğŸ“… ì™„ë£Œ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ“ˆ DB ì²­í¬ ë³€í™”: {initial_count} -> {final_count} (ì¦ë¶„: {total_added_chunks})")
    print(f"ğŸ“„ ì—ëŸ¬ ë¡œê·¸ í™•ì¸: {log_file_path}")
    
    # ë§ˆì§€ë§‰ ë°ì´í„° ìƒ˜í”Œ ê²€ì¦
    try:
        sample = vector_db.get(limit=1, include=['documents', 'metadatas'])
        if sample['documents']:
            print(f"ğŸ”— ê²€ì¦ ì¶œì²˜: {sample['metadatas'][0].get(Settings.META_SOURCE_KEY)}")
            print(f"ğŸ“ ë‚´ìš© ìƒ˜í”Œ: {sample['documents'][0][:50].replace('\n', ' ')}...")
    except: pass
    print("="*60)

if __name__ == "__main__":
    process_and_save()import os
import json
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import TextLoader
from config import Settings  # ì¤‘ì•™ ì„¤ì • ì°¸ì¡°

def process_and_save():
    # 1. DB ë° ëª¨ë¸ ì„¤ì • (ê¸°ì¡´ ê°’ ì£¼ì„ ë³´ì¡´)
    # DB_PATH = r"C:/Users/USER/rag/src/data/chroma_db"
    db_path = str(Settings.CHROMA_DB_PATH)
    # COLLECTION_NAME = "indonesia_pdt_docs"
    collection_name = Settings.CHROMA_COLLECTION_NAME
    # embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
    embeddings = OpenAIEmbeddings(model=Settings.EMBEDDING_MODEL)

    # 2. í…ìŠ¤íŠ¸ ë¶„í•  ì„¤ì • (ê¸°ì¡´ ê°’ ì£¼ì„ ë³´ì¡´)
    # text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=Settings.CHUNK_SIZE,
        chunk_overlap=Settings.CHUNK_OVERLAP
    )

    # 3. ë²¡í„° DB ì´ˆê¸°í™”/ì—°ê²°
    vector_db = Chroma(
        persist_directory=db_path,
        embedding_function=embeddings,
        collection_name=collection_name
    )

    # 4. ì‘ì—… ëŒ€ìƒ íŒŒì¼ ëª©ë¡ (configì˜ DATA_DIR ë‚´ text_converted í´ë” ê¸°ì¤€)
    input_dir = Settings.DATA_DIR / "text_converted"
    # state_file = "batch_state.json"
    state_file = Settings.BATCH_STATE_FILE

    # v4 ì´ì–´ë„£ê¸° ìƒíƒœ ë¡œë“œ
    processed_files = set()
    if os.path.exists(state_file):
        with open(state_file, "r", encoding="utf-8") as f:
            processed_files = set(json.load(f))

    all_files = [f for f in os.listdir(input_dir) if f.endswith(".txt")]
    
    for file_name in all_files:
        if file_name in processed_files:
            continue
            
        file_path = os.path.join(input_dir, file_name)
        try:
            loader = TextLoader(file_path, encoding='utf-8')
            raw_docs = loader.load()
            
            for doc in raw_docs:
                # 1. ë¬¸ì„œ ì „ì²´ ë‚´ìš©ì„ ì¤„ ë‹¨ìœ„ë¡œ ë¶„ë¦¬
                lines = doc.page_content.split('\n')
                first_line = lines[0].strip() # ì²« ì¤„ ì¶”ì¶œ
                
                # 2. "Source:" ë¬¸êµ¬ê°€ ìˆëŠ”ì§€ í™•ì¸í•˜ê³  íŒŒì‹±
                if first_line.startswith("Source:"):
                    # "Source:" ê¸€ì ìì²´ë¥¼ ê±·ì–´ë‚´ê³  ê²½ë¡œë§Œ ë‚¨ê¹€
                    full_path = first_line.replace("Source:", "").strip()
                    
                    # 3. [OS í†µí•©] ìœˆë„ìš°(\)ì™€ ë§¥(/) ê²½ë¡œ êµ¬ë¶„ìë¥¼ /ë¡œ í†µì¼í•˜ì—¬ ë§ˆì§€ë§‰ íŒŒì¼ëª… ì¶”ì¶œ
                    # ì´ë ‡ê²Œ í•´ì•¼ '...ì œì•ˆ ìš”ì•½ - 20241226-2-1.pdf' ì „ì²´ê°€ ì¡í™ë‹ˆë‹¤.
                    unified_path = full_path.replace('\\', '/')
                    original_name = unified_path.split('/')[-1]
                    
                    # 4. ë³¸ë¬¸ ì •ì œ: ì²« ì¤„(Source)ê³¼ ê·¸ ë‹¤ìŒ êµ¬ë¶„ì„ (---)ê¹Œì§€ ì œê±°
                    # ë³´í†µ 0, 1ë²ˆ ì¤„ì´ ë©”íƒ€ë°ì´í„°ì´ë¯€ë¡œ 2ë²ˆ ì¤„ë¶€í„° ë³¸ë¬¸ìœ¼ë¡œ ì‚¬ìš©
                    doc.page_content = "\n".join(lines[2:]).strip()
                else:
                    # ë§Œì•½ ì²« ì¤„ì— Sourceê°€ ì—†ë‹¤ë©´, ì°¨ì„ ì±…ìœ¼ë¡œ txt íŒŒì¼ëª…ì—ì„œ í•´ì‹œ ì œê±°
                    original_name = file_name.rsplit('_', 1)[0] + ".pdf" # í™•ì¥ì ê°•ì œ ë¶€ì—¬

                # 5. ìµœì¢… ê²°ì •ëœ 'ì›ë³¸ëª….pdf'ë¥¼ ë©”íƒ€ë°ì´í„°ì— ì£¼ì…
                doc.metadata[Settings.META_SOURCE_KEY] = original_name
            
            # ì²­í¬ ë¶„í•  ë° ì €ì¥
            final_chunks = text_splitter.split_documents(raw_docs)
            vector_db.add_documents(final_chunks)
            
            # ì§„í–‰ ìƒíƒœ ê¸°ë¡ (v4 ì´ì–´ë„£ê¸°)
            processed_files.add(file_name)
            with open(state_file, "w", encoding="utf-8") as f:
                json.dump(list(processed_files), f, ensure_ascii=False, indent=4)
            
            print(f"âœ… ì ì¬ ì™„ë£Œ: {file_name}")

        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜ ë°œìƒ ({file_name}): {e}")

if __name__ == "__main__":
    process_and_save()
import os
import logging
from pathlib import Path
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_text_splitters import RecursiveCharacterTextSplitter
from dotenv import load_dotenv
import time

# .env íŒŒì¼ ë¡œë“œ
load_dotenv() 

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

# í™•ì¸ìš©
if OPENAI_API_KEY:
    print(f"ğŸ”‘ API KEY ë¡œë“œ ì„±ê³µ: {OPENAI_API_KEY[:5]}*****")
else:
    print("âŒ .env íŒŒì¼ì—ì„œ OPENAI_API_KEYë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

# [ì„¤ì •] ê²½ë¡œëŠ” v3ì™€ ë™ì¼í•˜ê²Œ ìœ ì§€ (ì‚¬ìš©ìë‹˜ì´ ìœˆë„ìš°ì—ì„œ íŒŒì¼ë§Œ êµì²´)
TXT_DIR = Path(r"C:/Users/USER/rag/src/data/text_converted")
DB_PATH = r"C:/Users/USER/rag/src/data/chroma_db"
COLLECTION_NAME = "indonesia_pdt_docs"

def append_to_existing_db():
    # 1. ëª¨ë¸ ë° ìŠ¤í”Œë¦¬í„° ì„¤ì • (v3ì™€ ë™ì¼í•˜ê²Œ ìœ ì§€í•´ì•¼ ë°ì´í„° ì¼ê´€ì„± ë³´ì¥)
    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000, 
        chunk_overlap=150,
        separators=["\n\n", "\n", " ", ""]
    )

    # 2. [v4 í•µì‹¬] ê¸°ì¡´ DB ë¶ˆëŸ¬ì˜¤ê¸° (ì‚­ì œ ë¡œì§ ì—†ìŒ)
    if os.path.exists(DB_PATH):
        print(f"ğŸ“¦ ê¸°ì¡´ DB ë¡œë“œ ì¤‘: {DB_PATH}")
        vector_db = Chroma(
            persist_directory=DB_PATH,
            embedding_function=embeddings,
            collection_name=COLLECTION_NAME
        )
    else:
        print("âŒ ê¸°ì¡´ DBë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        return

    # 3. ìƒˆë¡œìš´ íŒŒì¼ ëª©ë¡ (3.2GB íŒŒì¼ë“¤ì´ ìˆëŠ” í´ë”)
    all_files = list(TXT_DIR.glob("*.txt"))
    print(f"ğŸš€ ì´ {len(all_files)}ê°œ íŒŒì¼ ì¶”ê°€ ì ì¬ ì‹œì‘...")

    # 4. ë°°ì¹˜ ì²˜ë¦¬ (ì‚¬ìš©ìë‹˜ ìµœì í™” ì„¤ì • ì ìš©)
    batch_size = 15  # íŒŒì¼ 15ê°œì”© ì½ê¸°
    for i in range(0, len(all_files), batch_size):
        batch_files = all_files[i : i + batch_size]
        texts = []
        metadatas = []

        for file_path in batch_files:
            try:
                with open(file_path, "r", encoding="utf-8") as f:
                    content = f.read()
                    chunks = text_splitter.split_text(content)
                    for chunk in chunks:
                        texts.append(chunk)
                        metadatas.append({"source": file_path.name})
            except Exception as e:
                print(f"âŒ ì˜¤ë¥˜ ({file_path.name}): {e}")

        # 5. DBì— ë°ì´í„° ì¶”ê°€ (ì•ˆì „í•œ ì¬ì‹œë„ ë¡œì§ í¬í•¨)
        if texts:
            text_batch_limit = 100 
            for j in range(0, len(texts), text_batch_limit):
                sub_texts = texts[j : j + text_batch_limit]
                sub_metadatas = metadatas[j : j + text_batch_limit]
                
                success = False
                while not success:
                    try:
                        # add_textsë¥¼ í†µí•´ ê¸°ì¡´ ì»¬ë ‰ì…˜ì— ì¶”ê°€
                        vector_db.add_texts(texts=sub_texts, metadatas=sub_metadatas)
                        
                        # ì‚¬ìš©ìë‹˜ ì„¤ì •ê°’: 0.2ì´ˆ íœ´ì‹
                        time.sleep(0.2) 
                        success = True
                    except Exception as e:
                        if "429" in str(e):
                            print("â³ ì†ë„ ì œí•œ(429) ê°ì§€. 10ì´ˆ ëŒ€ê¸° í›„ ë‹¤ì‹œ ì‹œë„í•©ë‹ˆë‹¤...")
                            time.sleep(10)
                        else:
                            print(f"âŒ ë°ì´í„° ì¶”ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                            break # ì¹˜ëª…ì  ì—ëŸ¬ ì‹œ ì¤‘ë‹¨
            
            print(f"âœ… ì¶”ê°€ ì™„ë£Œ: {min(i + batch_size, len(all_files))} / {len(all_files)}")

    print(f"ğŸ ëª¨ë“  ë°ì´í„° ì¶”ê°€ ì™„ë£Œ! ìœ„ì¹˜: {DB_PATH}")

if __name__ == "__main__":
    append_to_existing_db()
