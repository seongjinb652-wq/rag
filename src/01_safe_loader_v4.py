import os
import json
import shutil
import time
import logging
from datetime import datetime
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_text_splitters import RecursiveCharacterTextSplitter
from config import Settings  # ì¤‘ì•™ ì„¤ì • ì°¸ì¡°

# 1. ì—ëŸ¬ ë¡œê·¸ ì„¤ì •
log_file_path = Settings.LOGS_DIR / f"loader_error_{datetime.now().strftime('%Y%m%d')}.log"
logging.basicConfig(
    filename=log_file_path,
    level=logging.ERROR,
    format='%(asctime)s - %(levelname)s - %(message)s',
    encoding='utf-8'
)

def get_db_status(vector_db):
    try:
        return vector_db._collection.count()
    except Exception:
        return 0

def process_and_save():
    db_path = str(Settings.CHROMA_DB_PATH)
    state_file = Settings.BATCH_STATE_FILE
    input_dir = Settings.DATA_DIR / "text_converted"
    embeddings = OpenAIEmbeddings(model=Settings.EMBEDDING_MODEL)
    
    # [ì´ˆê¸°í™” ì ˆì°¨]
    if Settings.RESET_DB:
        now = datetime.now().strftime("%H:%M:%S")
        print(f"[{now}] âš ï¸ [ì´ˆê¸°í™” ëª¨ë“œ] ê¸°ì¡´ ë°ì´í„°ë¥¼ ì‚­ì œí•©ë‹ˆë‹¤.")
        if os.path.exists(db_path):
            shutil.rmtree(db_path)
        if os.path.exists(state_file):
            os.remove(state_file)
        print(f"[{now}] ğŸ—‘ï¸  DB ë° ìƒíƒœ íŒŒì¼ ì‚­ì œ ì™„ë£Œ.")

    # 2. ë²¡í„° DB ì—°ê²°
    vector_db = Chroma(
        persist_directory=db_path,
        embedding_function=embeddings,
        collection_name=Settings.CHROMA_COLLECTION_NAME
    )

    # 3. ìƒíƒœ í™•ì¸ (ì´ì–´ë„£ê¸°ìš©)
    initial_count = get_db_status(vector_db)
    processed_files = set()
    if not Settings.RESET_DB and os.path.exists(state_file):
        with open(state_file, "r", encoding="utf-8") as f:
            processed_files = set(json.load(f))

    # 4. ëŒ€ìƒ íŒŒì¼ ëª©ë¡ ì¶”ì¶œ
    all_files = [f for f in os.listdir(input_dir) if f.endswith(".txt")]
    files_to_process = [f for f in all_files if f not in processed_files]
    total_files = len(files_to_process)
    
    print(f"\nğŸ“Š [DB í˜„í™©] ê¸°ì¡´ ë°ì´í„°: {initial_count}ê±´")
    print(f"ğŸš€ [ì‘ì—… ì‹œì‘] ì²˜ë¦¬ ëŒ€ìƒ: {total_files}ê°œ íŒŒì¼ (ë¡œê·¸: {log_file_path.name})\n")

    # ìŠ¤í”Œë¦¬í„° ì„¤ì • (512 ê¶Œì¥)
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=Settings.CHUNK_SIZE,
        chunk_overlap=Settings.CHUNK_OVERLAP,
        separators=["\n\n", "\n", " ", ""]
    )

    # 5. ë©”ì¸ ì²˜ë¦¬ ë£¨í”„
    total_added_chunks = 0
    batch_display_size = 20 # 20ê°œ íŒŒì¼ë§ˆë‹¤ ë³´ê³ 
    
    for idx, file_name in enumerate(files_to_process, 1):
        file_path = os.path.join(input_dir, file_name)
        now_time = datetime.now().strftime("%H:%M:%S")
        
        try:
            with open(file_path, "r", encoding="utf-8", errors="ignore") as f:
                full_content = f.read()
            
            lines = full_content.split('\n')
            
            # ì¶œì²˜ ë³µì› ë° ë³¸ë¬¸ ì •ì œ
            if lines and lines[0].startswith("Source:"):
                full_source_path = lines[0].replace("Source:", "").strip()
                original_name = full_source_path.replace('\\', '/').split('/')[-1]
                content_body = "\n".join(lines[2:]).strip()
            else:
                original_name = file_name.rsplit('_', 1)[0]
                content_body = full_content

            # ì²­í¬ ìƒì„±
            chunks = text_splitter.split_text(content_body)
            num_chunks = len(chunks)
            metadatas = [{Settings.META_SOURCE_KEY: original_name} for _ in range(num_chunks)]

            # ---------------------------------------------------------
            # [í•µì‹¬] ë¶„í•  ì ì¬ ë¡œì§ - 100ê°œì”© ëŠì–´ì„œ ì „ì†¡
            # ---------------------------------------------------------
            chunk_batch_size = 100 
            for i in range(0, num_chunks, chunk_batch_size):
                batch_chunks = chunks[i : i + chunk_batch_size]
                batch_metadatas = metadatas[i : i + chunk_batch_size]
                
                success = False
                while not success:
                    try:
                        vector_db.add_texts(texts=batch_chunks, metadatas=batch_metadatas)
                        time.sleep(Settings.SLEEP_INTERVAL)
                        success = True
                    except Exception as e:
                        err_str = str(e)
                        if "429" in err_str or "Rate limit" in err_str:
                            print(f"\n[{now_time}] â³ [Rate Limit] 15ì´ˆ ëŒ€ê¸°... ({idx}/{total_files})")
                            time.sleep(15)
                        elif "400" in err_str or "max_tokens" in err_str:
                            # 100ê°œë„ í¬ë©´ 20ê°œì”© ë” ì˜ê²Œ ìª¼ê°œì„œ ì¬ì‹œë„
                            print(f"\n[{now_time}] âš ï¸ [Token Limit] {original_name} ì¬ë¶„í•  ì ì¬ ì¤‘...")
                            for j in range(0, len(batch_chunks), 20):
                                vector_db.add_texts(texts=batch_chunks[j:j+20], metadatas=batch_metadatas[j:j+20])
                                time.sleep(Settings.SLEEP_INTERVAL)
                            success = True
                        else:
                            raise e

            # ìƒíƒœ ì—…ë°ì´íŠ¸
            total_added_chunks += num_chunks
            processed_files.add(file_name)
            with open(state_file, "w", encoding="utf-8") as f:
                json.dump(list(processed_files), f, ensure_ascii=False, indent=4)
            
            # ì§„í–‰ ë³´ê³ 
            if num_chunks >= 50: # ëŒ€í˜• íŒŒì¼ ê¸°ì¤€ ìƒí–¥
                print(f"\n[{now_time}] ğŸ˜ [ëŒ€í˜•] ({idx}/{total_files}) {original_name} (ì²­í¬: {num_chunks}ê°œ)")
            
            if idx % batch_display_size == 0:
                print(f"\n[{now_time}] ğŸ“¦ [ë°°ì¹˜] {idx}/{total_files} íŒŒì¼ ì™„ë£Œ (ëˆ„ì  ì²­í¬: {total_added_chunks})")
            else:
                print(f"\r[{now_time}] ({idx}/{total_files}) ì²˜ë¦¬ ì¤‘: {original_name[:25]}...", end="")

        except Exception as e:
            err_msg = f"ì‹¤íŒ¨: {file_name} | ì´ìœ : {str(e)}"
            print(f"\n[{now_time}] âŒ {err_msg}")
            logging.error(err_msg)

    # 6. ìµœì¢… ê²°ê³¼
    final_count = get_db_status(vector_db)
    print("\n\n" + "="*60)
    print("ğŸ ëª¨ë“  ë°ì´í„° ì ì¬ ì™„ë£Œ")
    print(f"ğŸ“ˆ DB ì²­í¬ ë³€í™”: {initial_count} -> {final_count} (ì¦ë¶„: {total_added_chunks})")
    print(f"ğŸ“„ ì—ëŸ¬ ë¡œê·¸: {log_file_path}")
    print("="*60)

if __name__ == "__main__":
    process_and_save()
