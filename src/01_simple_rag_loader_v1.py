#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import logging
import hashlib
import re
import zlib
import fitz  # PyMuPDF
from pathlib import Path
from datetime import datetime
from typing import List, Dict

# ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ
import chromadb
from openai import OpenAI
from docx import Document
from pptx import Presentation
import olefile  # pip install olefile í•„ìˆ˜

# 1. í™˜ê²½ ì„¤ì •
TARGET_DIR = Path(r"C:/Users/USER/Downloads/@@@ì¸ë„ë„¤ì‹œì•„PDTì•”ì„¼í„°FS")
DB_PATH = Path(r"C:/Users/USER/rag/src/data/chroma_db")
COLLECTION_NAME = "indonesia_pdt_docs"

# OpenAI ì„¤ì •
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY") 
EMBED_MODEL = "text-embedding-3-small"

CHUNK_SIZE = 800
CHUNK_OVERLAP = 100

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class RobustRAGLoaderV1:
    def __init__(self):
        logger.info("ğŸš€ ì‹œìŠ¤í…œ ì´ˆê¸°í™” (OpenAI Embedding Mode + HWP Support)")
        self.client = OpenAI(api_key=OPENAI_API_KEY)
        
        # ChromaDB ì´ˆê¸°í™”
        self.db_client = chromadb.PersistentClient(path=str(DB_PATH))
        
        # ì´ì–´ì„œ í•˜ê¸°(Append)ë¥¼ ìœ„í•´ get_or_create ì‚¬ìš©
        self.collection = self.db_client.get_or_create_collection(
            name=COLLECTION_NAME,
            metadata={"hnsw:space": "cosine"}
        )
        
    def extract_hwp_text(self, file_path: Path) -> str:
        import olefile
        import zlib
        import re

        text = ""
        try:
            f = olefile.OleFileIO(str(file_path))
            dirs = f.listdir()
            if ["FileHeader"] not in dirs: return ""

            bodytext = [d for d in dirs if d[0].startswith("BodyText")]
            for section in bodytext:
                data = f.openstream(section).read()
                try:
                    decompressed = zlib.decompress(data, -15)
                except:
                    decompressed = data
                
                # ê¹¨ë—í•œ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œí•˜ê¸° ìœ„í•œ ì²˜ë¦¬
                raw_content = decompressed.decode('utf-16', errors='ignore')
                
                # [í•µì‹¬] í•œê¸€, ì˜ë¬¸, ìˆ«ì, ì¼ë°˜ ë¬¸ì¥ë¶€í˜¸ë§Œ ë‚¨ê¸°ê³  ì œê±°
                clean_content = re.sub(r'[^\w\s\.\,\?\!\(\)\[\]\%\:\-\d\uAC00-\uD7A3]+', '', raw_content)
                text += clean_content
            f.close()
        except Exception as e:
            logger.error(f"âŒ {file_path.name} HWP ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        return text
    

    def extract_text(self, file_path: Path) -> str:
        """íŒŒì¼ë³„ í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ë³´ì •"""
        ext = file_path.suffix.lower()
        text = ""
        try:
            if ext == '.pdf':
                doc = fitz.open(file_path)
                text = " ".join([page.get_text() for page in doc])
                doc.close()
            elif ext == '.docx':
                doc = Document(file_path)
                text = "\n".join([p.text for p in doc.paragraphs])
            elif ext == '.hwp':
                text = self.extract_hwp_text(file_path)
            elif ext == '.pptx':
                prs = Presentation(file_path)
                text = " ".join([shape.text for slide in prs.slides for shape in slide.shapes if hasattr(shape, "text")])
            elif ext == '.txt':
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    text = f.read()

            if text:
                # 1. í•œê¸€ ì¸ì½”ë”© ì˜¤ë¥˜ êµì •
                corrections = {
                    "ì‹": "ì‹ ", "ì§‚": "ì§„", "ì¶": "ì¸", "í•š": "í•œ", 
                    "ì›": "ì›", "ìƒí™–": "ìƒí™˜", "ì·": "ì¼", "ì ‚": "ì „",
                    "ê³¾": "ê´€", "ì²š": "ì²œ", "ì¤‘ê°‚": "ì¤‘ê°„", "ì–¶": "ì–¸",
                    "ì»": "ì¸", "ì‹µ": "ì‹¤", "ì—“": "ì—‘", "ì§—": "ì§€",
                    "ìƒ": "ì€", "ì”": "ì–µ", "ì‹²": "ì‹œ", "ì§˜": "ì§"
                }
                for wrong, right in corrections.items():
                    text = text.replace(wrong, right)
                
                # 2. ì¤‘ë³µ ë‹¨ì–´ ì œê±°
                text = re.sub(r'([ê°€-í£]{2,})\1', r'\1', text)

        except Exception as e:
            logger.error(f"âŒ {file_path.name} ì²˜ë¦¬ ì—ëŸ¬: {e}")
        
        return text

    def run(self):
        # 1. ëª¨ë“  íŒŒì¼ ìŠ¤ìº” (HWP ì¶”ê°€)
        all_files = []
        target_exts = {'.pdf', '.docx', '.pptx', '.txt', '.hwp'}
        for root, _, filenames in os.walk(TARGET_DIR):
            for f in filenames:
                p = Path(root) / f
                if p.suffix.lower() in target_exts:
                    all_files.append(p)

        logger.info(f"ğŸ” ì´ {len(all_files)}ê°œ íŒŒì¼ ë°œê²¬")

        # 2. ì´ë¯¸ DBì— ì €ì¥ëœ íŒŒì¼ í™•ì¸
        existing_data = self.collection.get(include=['metadatas'])
        processed_files = {m['source'] for m in existing_data['metadatas']} if existing_data['metadatas'] else set()

        total_chunks = 0
        for idx, file_path in enumerate(all_files, 1):
            if file_path.name in processed_files:
                continue

            raw_text = self.extract_text(file_path)
            if not raw_text.strip():
                continue

            chunks = [raw_text[i:i+CHUNK_SIZE] for i in range(0, len(raw_text), CHUNK_SIZE - CHUNK_OVERLAP)]
            
            try:
                response = self.client.embeddings.create(input=chunks, model=EMBED_MODEL)
                embeddings = [data.embedding for data in response.data]
                
                ids = [hashlib.md5(f"{file_path.name}_{i}".encode()).hexdigest() for i in range(len(chunks))]
                self.collection.add(
                    ids=ids,
                    embeddings=embeddings,
                    documents=chunks,
                    metadatas=[{"source": file_path.name} for _ in chunks]
                )
                total_chunks += len(chunks)
                logger.info(f"âœ… [{idx}/{len(all_files)}] {file_path.name} ({len(chunks)} Chunks)")
            
            except Exception as e:
                logger.error(f"âŒ [{idx}/{len(all_files)}] {file_path.name} ì„ë² ë”© ì—ëŸ¬: {e}")

        logger.info(f"ğŸ ì™„ë£Œ! í˜„ì¬ ì»¬ë ‰ì…˜ ë‚´ ì´ ì²­í¬ ìˆ˜: {self.collection.count()}")

if __name__ == "__main__":
    loader = RobustRAGLoaderV1()
    loader.run()
